Iteration 1, loss = 1.08244789
Iteration 2, loss = 0.94750774
Iteration 3, loss = 0.88911307
Iteration 4, loss = 0.85182583
Iteration 5, loss = 0.82016596
Iteration 6, loss = 0.79192367
Iteration 7, loss = 0.76646656
Iteration 8, loss = 0.74538186
Iteration 9, loss = 0.72401391
Iteration 10, loss = 0.71162424
Iteration 11, loss = 0.70263282
Iteration 12, loss = 0.68369484
Iteration 13, loss = 0.66656139
Iteration 14, loss = 0.65627578
Iteration 15, loss = 0.64183832
Iteration 16, loss = 0.62998557
Iteration 17, loss = 0.62050569
Iteration 18, loss = 0.61406718
Iteration 19, loss = 0.60172605
Iteration 20, loss = 0.61131762
Iteration 21, loss = 0.59472513
Iteration 22, loss = 0.57977393
Iteration 23, loss = 0.57163595
Iteration 24, loss = 0.56366032
Iteration 25, loss = 0.55921685
Iteration 26, loss = 0.55135519
Iteration 27, loss = 0.54461696
Iteration 28, loss = 0.54587492
Iteration 29, loss = 0.53661919
Iteration 30, loss = 0.52843699
Iteration 31, loss = 0.52582946
Iteration 32, loss = 0.52165207
Iteration 33, loss = 0.51166559
Iteration 34, loss = 0.50846451
Iteration 35, loss = 0.50484963
Iteration 36, loss = 0.49984028
Iteration 37, loss = 0.49946115
Iteration 38, loss = 0.49286768
Iteration 39, loss = 0.48983251
Iteration 40, loss = 0.48365210
Iteration 41, loss = 0.47917658
Iteration 42, loss = 0.47467323
Iteration 43, loss = 0.47087850
Iteration 44, loss = 0.47028059
Iteration 45, loss = 0.46893682
Iteration 46, loss = 0.46136801
Iteration 47, loss = 0.46193641
Iteration 48, loss = 0.45452939
Iteration 49, loss = 0.45150799
Iteration 50, loss = 0.44845000
Iteration 51, loss = 0.44404022
Iteration 52, loss = 0.44135302
Iteration 53, loss = 0.43975218
Iteration 54, loss = 0.44054968
Iteration 55, loss = 0.43441815
Iteration 56, loss = 0.43172380
Iteration 57, loss = 0.43641058
Iteration 58, loss = 0.43311677
Iteration 59, loss = 0.42646086
Iteration 60, loss = 0.43116904
Iteration 61, loss = 0.42501205
Iteration 62, loss = 0.41813730
Iteration 63, loss = 0.42890131
Iteration 64, loss = 0.41612070
Iteration 65, loss = 0.41220203
Iteration 66, loss = 0.41177863
Iteration 67, loss = 0.40809825
Iteration 68, loss = 0.40602246
Iteration 69, loss = 0.40701856
Iteration 70, loss = 0.40507989
Iteration 71, loss = 0.40264504
Iteration 72, loss = 0.40117243
Iteration 73, loss = 0.40034760
Iteration 74, loss = 0.39715972
Iteration 75, loss = 0.39562964
Iteration 76, loss = 0.39377917
Iteration 77, loss = 0.39323988
Iteration 78, loss = 0.40736276
Iteration 79, loss = 0.39539870
Iteration 80, loss = 0.39260672
Iteration 81, loss = 0.39057810
Iteration 82, loss = 0.39115764
Iteration 83, loss = 0.38612467
Iteration 84, loss = 0.38775221
Iteration 85, loss = 0.38714365
Iteration 86, loss = 0.38419318
Iteration 87, loss = 0.38327182
Iteration 88, loss = 0.38191800
Iteration 89, loss = 0.38181914
Iteration 90, loss = 0.38183236
Iteration 91, loss = 0.37933119
Iteration 92, loss = 0.37918892
Iteration 93, loss = 0.37880908
Iteration 94, loss = 0.37565026
Iteration 95, loss = 0.37450365
Iteration 96, loss = 0.37179303
Iteration 97, loss = 0.37078157
Iteration 98, loss = 0.38144606
Iteration 99, loss = 0.37178265
Iteration 100, loss = 0.36967333
Iteration 101, loss = 0.37071676
Iteration 102, loss = 0.36944700
Iteration 103, loss = 0.37416670
Iteration 104, loss = 0.38061395
Iteration 105, loss = 0.36961216
Iteration 106, loss = 0.36498016
Iteration 107, loss = 0.36398482
Iteration 108, loss = 0.37359842
Iteration 109, loss = 0.36451131
Iteration 110, loss = 0.36351074
Iteration 111, loss = 0.36137144
Iteration 112, loss = 0.36606122
Iteration 113, loss = 0.36121444
Iteration 114, loss = 0.35991630
Iteration 115, loss = 0.36323196
Iteration 116, loss = 0.36610524
Iteration 117, loss = 0.36047731
Iteration 118, loss = 0.37194340
Iteration 119, loss = 0.36286183
Iteration 120, loss = 0.35905052
Iteration 121, loss = 0.36172271
Iteration 122, loss = 0.35680634
Iteration 123, loss = 0.35558884
Iteration 124, loss = 0.35637718
Iteration 125, loss = 0.35574568
Iteration 126, loss = 0.35808901
Iteration 127, loss = 0.35531453
Iteration 128, loss = 0.35508216
Iteration 129, loss = 0.35243448
Iteration 130, loss = 0.35196405
Iteration 131, loss = 0.35298836
Iteration 132, loss = 0.35388342
Iteration 133, loss = 0.35135048
Iteration 134, loss = 0.35149712
Iteration 135, loss = 0.35160994
Iteration 136, loss = 0.35020205
Iteration 137, loss = 0.34940048
Iteration 138, loss = 0.35378773
Iteration 139, loss = 0.35702757
Iteration 140, loss = 0.35091185
Iteration 141, loss = 0.38027836
Iteration 142, loss = 0.35906797
Iteration 143, loss = 0.35744010
Iteration 144, loss = 0.36195683
Iteration 145, loss = 0.35327760
Iteration 146, loss = 0.35812601
Iteration 147, loss = 0.34900564
Iteration 148, loss = 0.36328250
Iteration 149, loss = 0.35368227
Iteration 150, loss = 0.34623269
Iteration 151, loss = 0.34492579
Iteration 152, loss = 0.35260397
Iteration 153, loss = 0.34847262
Iteration 154, loss = 0.34954074
Iteration 155, loss = 0.34919270
Iteration 156, loss = 0.34433656
Iteration 157, loss = 0.34656044
Iteration 158, loss = 0.34675669
Iteration 159, loss = 0.34667596
Iteration 160, loss = 0.34411367
Iteration 161, loss = 0.34704582
Iteration 162, loss = 0.34608721
Iteration 163, loss = 0.34649644
Iteration 164, loss = 0.34860076
Iteration 165, loss = 0.34448353
Iteration 166, loss = 0.34344530
Iteration 167, loss = 0.35506521
Iteration 168, loss = 0.34490175
Iteration 169, loss = 0.34669008
Iteration 170, loss = 0.35700130
Iteration 171, loss = 0.34580374
Iteration 172, loss = 0.34254144
Iteration 173, loss = 0.34369486
Iteration 174, loss = 0.34664957
Iteration 175, loss = 0.34212270
Iteration 176, loss = 0.34388923
Iteration 177, loss = 0.34056108
Iteration 178, loss = 0.34391587
Iteration 179, loss = 0.34208340
Iteration 180, loss = 0.33918503
Iteration 181, loss = 0.35131116
Iteration 182, loss = 0.34250221
Iteration 183, loss = 0.34038263
Iteration 184, loss = 0.35058208
Iteration 185, loss = 0.34163723
Iteration 186, loss = 0.34338198
Iteration 187, loss = 0.35557714
Iteration 188, loss = 0.34094472
Iteration 189, loss = 0.33919420
Iteration 190, loss = 0.33847185
Iteration 191, loss = 0.33713700
Iteration 192, loss = 0.33842942
Iteration 193, loss = 0.33749354
Iteration 194, loss = 0.34653081
Iteration 195, loss = 0.34278569
Iteration 196, loss = 0.34749803
Iteration 197, loss = 0.33952713
Iteration 198, loss = 0.34516277
Iteration 199, loss = 0.35608397
Iteration 200, loss = 0.34844402
Iteration 201, loss = 0.34041372
Iteration 202, loss = 0.34137871
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 3/5] END ...........alpha=0.2, momentum=0.4;, score=0.881 total time=   6.5s
Iteration 1, loss = 1.12342895
Iteration 2, loss = 0.98717066
Iteration 3, loss = 0.93154161
Iteration 4, loss = 0.89677371
Iteration 5, loss = 0.86796353
Iteration 6, loss = 0.84223147
Iteration 7, loss = 0.81924876
Iteration 8, loss = 0.80038398
Iteration 9, loss = 0.78142717
Iteration 10, loss = 0.77189498
Iteration 11, loss = 0.76363980
Iteration 12, loss = 0.74695176
Iteration 13, loss = 0.73201161
Iteration 14, loss = 0.72433163
Iteration 15, loss = 0.71107593
Iteration 16, loss = 0.70100152
Iteration 17, loss = 0.69336432
Iteration 18, loss = 0.68950609
Iteration 19, loss = 0.67868277
Iteration 20, loss = 0.68726977
Iteration 21, loss = 0.67494350
Iteration 22, loss = 0.66140863
Iteration 23, loss = 0.65484953
Iteration 24, loss = 0.64832287
Iteration 25, loss = 0.64457902
Iteration 26, loss = 0.63852100
Iteration 27, loss = 0.63353029
Iteration 28, loss = 0.63584514
Iteration 29, loss = 0.62774059
Iteration 30, loss = 0.62093704
Iteration 31, loss = 0.61941786
Iteration 32, loss = 0.61592454
Iteration 33, loss = 0.60727093
Iteration 34, loss = 0.60501823
Iteration 35, loss = 0.60270764
Iteration 36, loss = 0.59893022
Iteration 37, loss = 0.59819160
Iteration 38, loss = 0.59365703
Iteration 39, loss = 0.59058829Iteration 1, loss = 1.08113969
Iteration 2, loss = 0.94805588
Iteration 3, loss = 0.89546675
Iteration 4, loss = 0.85793742
Iteration 5, loss = 0.82629067
Iteration 6, loss = 0.80647571
Iteration 7, loss = 0.78453735
Iteration 8, loss = 0.76450371
Iteration 9, loss = 0.74663816
Iteration 10, loss = 0.73659157
Iteration 11, loss = 0.72622771
Iteration 12, loss = 0.71206501
Iteration 13, loss = 0.72318323
Iteration 14, loss = 0.69245901
Iteration 15, loss = 0.68868758
Iteration 16, loss = 0.68172477
Iteration 17, loss = 0.66489526
Iteration 18, loss = 0.66476639
Iteration 19, loss = 0.65475146
Iteration 20, loss = 0.64732497
Iteration 21, loss = 0.63850256
Iteration 22, loss = 0.63650816
Iteration 23, loss = 0.63467347
Iteration 24, loss = 0.63528293
Iteration 25, loss = 0.62308191
Iteration 26, loss = 0.62413277
Iteration 27, loss = 0.61308400
Iteration 28, loss = 0.60677076
Iteration 29, loss = 0.62185416
Iteration 30, loss = 0.60285890
Iteration 31, loss = 0.59575211
Iteration 32, loss = 0.58944573
Iteration 33, loss = 0.58443942
Iteration 34, loss = 0.58267757
Iteration 35, loss = 0.58196489
Iteration 36, loss = 0.57942553
Iteration 37, loss = 0.57158933
Iteration 38, loss = 0.57063231
Iteration 39, loss = 0.56975882
Iteration 40, loss = 0.56124073
Iteration 41, loss = 0.55753992
Iteration 42, loss = 0.55346212
Iteration 43, loss = 0.54986297
Iteration 44, loss = 0.56223409
Iteration 45, loss = 0.55729600
Iteration 46, loss = 0.54897866
Iteration 47, loss = 0.54535870
Iteration 48, loss = 0.54583691
Iteration 49, loss = 0.55108444
Iteration 50, loss = 0.54340437
Iteration 51, loss = 0.54776148
Iteration 52, loss = 0.55218086
Iteration 53, loss = 0.53742695
Iteration 54, loss = 0.52751360
Iteration 55, loss = 0.52867607
Iteration 56, loss = 0.53523514
Iteration 57, loss = 0.52293935
Iteration 58, loss = 0.52474866
Iteration 59, loss = 0.51632123
Iteration 60, loss = 0.51523757
Iteration 61, loss = 0.51313581
Iteration 62, loss = 0.56257944
Iteration 63, loss = 0.52149380
Iteration 64, loss = 0.51610134
Iteration 65, loss = 0.53229737
Iteration 66, loss = 0.51472415
Iteration 67, loss = 0.51355178
Iteration 68, loss = 0.50612957
Iteration 69, loss = 0.50488311
Iteration 70, loss = 0.50918063
Iteration 71, loss = 0.49904544
Iteration 72, loss = 0.49909374
Iteration 73, loss = 0.50156692
Iteration 74, loss = 0.51276423
Iteration 75, loss = 0.50013200
Iteration 76, loss = 0.49285578
Iteration 77, loss = 0.48962590
Iteration 78, loss = 0.49473602
Iteration 79, loss = 0.48800125
Iteration 80, loss = 0.48509741
Iteration 81, loss = 0.48609553
Iteration 82, loss = 0.48468939
Iteration 83, loss = 0.48584914
Iteration 84, loss = 0.48054610
Iteration 85, loss = 0.47858590
Iteration 86, loss = 0.48161343
Iteration 87, loss = 0.48694729
Iteration 88, loss = 0.47926638
Iteration 89, loss = 0.47451997
Iteration 90, loss = 0.47813572
Iteration 91, loss = 0.49142343
Iteration 92, loss = 0.47525398
Iteration 93, loss = 0.47167972
Iteration 94, loss = 0.47890434
Iteration 95, loss = 0.49804306
Iteration 96, loss = 0.47356947
Iteration 97, loss = 0.46765906
Iteration 98, loss = 0.46985650
Iteration 99, loss = 0.50578954
Iteration 100, loss = 0.48006500
Iteration 101, loss = 0.47366067
Iteration 102, loss = 0.47567541
Iteration 103, loss = 0.46802381
Iteration 104, loss = 0.46583754
Iteration 105, loss = 0.46194295
Iteration 106, loss = 0.46281207
Iteration 107, loss = 0.46374716
Iteration 108, loss = 0.46469365
Iteration 109, loss = 0.46025083
Iteration 110, loss = 0.45646076
Iteration 111, loss = 0.45824614
Iteration 112, loss = 0.45492153
Iteration 113, loss = 0.45693790
Iteration 114, loss = 0.45395384
Iteration 115, loss = 0.45229440
Iteration 116, loss = 0.45675835
Iteration 117, loss = 0.45255118
Iteration 118, loss = 0.45918612
Iteration 119, loss = 0.46191750
Iteration 120, loss = 0.46296991
Iteration 121, loss = 0.45262116
Iteration 122, loss = 0.46303562
Iteration 123, loss = 0.45706574
Iteration 124, loss = 0.45088923
Iteration 125, loss = 0.46203286
Iteration 126, loss = 0.44913323
Iteration 127, loss = 0.44575078
Iteration 128, loss = 0.45025626
Iteration 129, loss = 0.44470937
Iteration 130, loss = 0.44455272
Iteration 131, loss = 0.44574117
Iteration 132, loss = 0.44694039
Iteration 133, loss = 0.44320600
Iteration 134, loss = 0.44589602
Iteration 135, loss = 0.44883379
Iteration 136, loss = 0.44418159
Iteration 137, loss = 0.44238977
Iteration 138, loss = 0.44604647
Iteration 139, loss = 0.44595239
Iteration 140, loss = 0.44176347
Iteration 141, loss = 0.43911152
Iteration 142, loss = 0.44602736
Iteration 143, loss = 0.44454028
Iteration 144, loss = 0.43829575
Iteration 145, loss = 0.43607618
Iteration 146, loss = 0.45466078
Iteration 147, loss = 0.45693802
Iteration 148, loss = 0.44090243
Iteration 149, loss = 0.43691064
Iteration 150, loss = 0.43743626
Iteration 151, loss = 0.43462753
Iteration 152, loss = 0.44563560
Iteration 153, loss = 0.43712724
Iteration 154, loss = 0.43417480
Iteration 155, loss = 0.43520894
Iteration 156, loss = 0.44027034
Iteration 157, loss = 0.43573519
Iteration 158, loss = 0.43410361
Iteration 159, loss = 0.43262089
Iteration 160, loss = 0.43495912
Iteration 161, loss = 0.44154481
Iteration 162, loss = 0.43713703
Iteration 163, loss = 0.43385531
Iteration 164, loss = 0.43125900
Iteration 165, loss = 0.43307586
Iteration 166, loss = 0.44358495
Iteration 167, loss = 0.43460934
Iteration 168, loss = 0.43482286
Iteration 169, loss = 0.43089898
Iteration 170, loss = 0.44697129
Iteration 171, loss = 0.44143557
Iteration 172, loss = 0.45528416
Iteration 173, loss = 0.43419469
Iteration 174, loss = 0.43953496
Iteration 175, loss = 0.43136766
Iteration 176, loss = 0.42705614
Iteration 177, loss = 0.43127469
Iteration 178, loss = 0.43101347
Iteration 179, loss = 0.42789167
Iteration 180, loss = 0.44043424
Iteration 181, loss = 0.43026591
Iteration 182, loss = 0.43310838
Iteration 183, loss = 0.43448339
Iteration 184, loss = 0.42979747
Iteration 185, loss = 0.42830497
Iteration 186, loss = 0.42776379
Iteration 187, loss = 0.42921738
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 2/5] END ...........alpha=0.2, momentum=0.4;, score=0.861 total time=   6.0s
Iteration 1, loss = 1.12216192
Iteration 2, loss = 0.98959983
Iteration 3, loss = 0.94225857
Iteration 4, loss = 0.90898391
Iteration 5, loss = 0.88157569
Iteration 6, loss = 0.86506331
Iteration 7, loss = 0.84630254
Iteration 8, loss = 0.82919003
Iteration 9, loss = 0.81440589
Iteration 10, loss = 0.80690565
Iteration 11, loss = 0.79971090
Iteration 12, loss = 0.78809196
Iteration 13, loss = 0.80231225
Iteration 14, loss = 0.77280756
Iteration 15, loss = 0.76922926
Iteration 16, loss = 0.76529946
Iteration 17, loss = 0.74980269
Iteration 18, loss = 0.75357005
Iteration 19, loss = 0.74389785
Iteration 20, loss = 0.73831562
Iteration 21, loss = 0.73119849
Iteration 22, loss = 0.72924797
Iteration 23, loss = 0.72795678
Iteration 24, loss = 0.72957083
Iteration 25, loss = 0.72108647
Iteration 26, loss = 0.72500097
Iteration 27, loss = 0.71509327
Iteration 28, loss = 0.71203816
Iteration 29, loss = 0.72349669
Iteration 30, loss = 0.70881886
Iteration 31, loss = 0.70414684
Iteration 32, loss = 0.69781700
Iteration 33, loss = 0.69402968
Iteration 34, loss = 0.69425439
Iteration 35, loss = 0.69489769
Iteration 36, loss = 0.69380916
Iteration 37, loss = 0.68646718
Iteration 38, loss = 0.68757476
Iteration 39, loss = 0.68788324
Iteration 40, loss = 0.68034101
Iteration 41, loss = 0.67737616
Iteration 42, loss = 0.67478311
Iteration 43, loss = 0.67230949
Iteration 44, loss = 0.68503469
Iteration 45, loss = 0.67609487
Iteration 46, loss = 0.67743455
Iteration 47, loss = 0.67085663
Iteration 48, loss = 0.67038582
Iteration 49, loss = 0.67453629
Iteration 50, loss = 0.67677529
Iteration 51, loss = 0.67649187
Iteration 52, loss = 0.67550394
Iteration 53, loss = 0.66756210
Iteration 54, loss = 0.65999849Iteration 1, loss = 1.08672033
Iteration 2, loss = 0.95326742
Iteration 3, loss = 0.89964341
Iteration 4, loss = 0.86187684
Iteration 5, loss = 0.84308703
Iteration 6, loss = 0.81711724
Iteration 7, loss = 0.79424847
Iteration 8, loss = 0.77647873
Iteration 9, loss = 0.75982537
Iteration 10, loss = 0.74935440
Iteration 11, loss = 0.74006730
Iteration 12, loss = 0.72717435
Iteration 13, loss = 0.73951893
Iteration 14, loss = 0.70619259
Iteration 15, loss = 0.69421424
Iteration 16, loss = 0.68993870
Iteration 17, loss = 0.67580203
Iteration 18, loss = 0.67793620
Iteration 19, loss = 0.66512720
Iteration 20, loss = 0.65851052
Iteration 21, loss = 0.64833575
Iteration 22, loss = 0.64224693
Iteration 23, loss = 0.64550038
Iteration 24, loss = 0.64378438
Iteration 25, loss = 0.63353248
Iteration 26, loss = 0.63250402
Iteration 27, loss = 0.62235617
Iteration 28, loss = 0.61424484
Iteration 29, loss = 0.62407076
Iteration 30, loss = 0.60813963
Iteration 31, loss = 0.60421558
Iteration 32, loss = 0.59698483
Iteration 33, loss = 0.59135342
Iteration 34, loss = 0.58973103
Iteration 35, loss = 0.58987198
Iteration 36, loss = 0.58191757
Iteration 37, loss = 0.57613900
Iteration 38, loss = 0.57184072
Iteration 39, loss = 0.57029678
Iteration 40, loss = 0.56532331
Iteration 41, loss = 0.56728893
Iteration 42, loss = 0.56214205
Iteration 43, loss = 0.55709564
Iteration 44, loss = 0.57088247
Iteration 45, loss = 0.56593239
Iteration 46, loss = 0.55646089
Iteration 47, loss = 0.55219528
Iteration 48, loss = 0.54876832
Iteration 49, loss = 0.55611838
Iteration 50, loss = 0.54923301
Iteration 51, loss = 0.54523099
Iteration 52, loss = 0.55662707
Iteration 53, loss = 0.54068768
Iteration 54, loss = 0.53197698
Iteration 55, loss = 0.52725727
Iteration 56, loss = 0.54210889
Iteration 57, loss = 0.52586080
Iteration 58, loss = 0.53000874
Iteration 59, loss = 0.52001473
Iteration 60, loss = 0.51729479
Iteration 61, loss = 0.51740618
Iteration 62, loss = 0.55556022
Iteration 63, loss = 0.51950551
Iteration 64, loss = 0.51118963
Iteration 65, loss = 0.53387665
Iteration 66, loss = 0.51718516
Iteration 67, loss = 0.51370356
Iteration 68, loss = 0.50761998
Iteration 69, loss = 0.50582953
Iteration 70, loss = 0.51808205
Iteration 71, loss = 0.50267305
Iteration 72, loss = 0.50108390
Iteration 73, loss = 0.50179216
Iteration 74, loss = 0.49544169
Iteration 75, loss = 0.49989972
Iteration 76, loss = 0.49389043
Iteration 77, loss = 0.49088458
Iteration 78, loss = 0.49285381
Iteration 79, loss = 0.49213456
Iteration 80, loss = 0.48729336
Iteration 81, loss = 0.48867929
Iteration 82, loss = 0.48700482
Iteration 83, loss = 0.50580148
Iteration 84, loss = 0.49160838
Iteration 85, loss = 0.48433106
Iteration 86, loss = 0.48554094
Iteration 87, loss = 0.49060969
Iteration 88, loss = 0.48036576
Iteration 89, loss = 0.47569344
Iteration 90, loss = 0.47571468
Iteration 91, loss = 0.47597991
Iteration 92, loss = 0.47322994
Iteration 93, loss = 0.47126879
Iteration 94, loss = 0.46895902
Iteration 95, loss = 0.48767074
Iteration 96, loss = 0.47128551
Iteration 97, loss = 0.46759547
Iteration 98, loss = 0.47152284
Iteration 99, loss = 0.51034400
Iteration 100, loss = 0.48094875
Iteration 101, loss = 0.47908186
Iteration 102, loss = 0.48520669
Iteration 103, loss = 0.47183434
Iteration 104, loss = 0.46715131
Iteration 105, loss = 0.46408979
Iteration 106, loss = 0.46250694
Iteration 107, loss = 0.46569348
Iteration 108, loss = 0.46453917
Iteration 109, loss = 0.46039268
Iteration 110, loss = 0.45788518
Iteration 111, loss = 0.45975400
Iteration 112, loss = 0.45604065
Iteration 113, loss = 0.45853749
Iteration 114, loss = 0.45524113
Iteration 115, loss = 0.45439972
Iteration 116, loss = 0.45443578
Iteration 117, loss = 0.45205553
Iteration 118, loss = 0.45517675
Iteration 119, loss = 0.46382129
Iteration 120, loss = 0.46113942
Iteration 121, loss = 0.45468329
Iteration 122, loss = 0.46518130
Iteration 123, loss = 0.45835111
Iteration 124, loss = 0.45256713
Iteration 125, loss = 0.45954614
Iteration 126, loss = 0.45117391
Iteration 127, loss = 0.44698919
Iteration 128, loss = 0.45353949
Iteration 129, loss = 0.45687701
Iteration 130, loss = 0.44974448
Iteration 131, loss = 0.44755897
Iteration 132, loss = 0.44580524
Iteration 133, loss = 0.44445476
Iteration 134, loss = 0.45238777
Iteration 135, loss = 0.44478346
Iteration 136, loss = 0.44415850
Iteration 137, loss = 0.44129760
Iteration 138, loss = 0.44768830
Iteration 139, loss = 0.45425911
Iteration 140, loss = 0.44733878
Iteration 141, loss = 0.44323266
Iteration 142, loss = 0.44688965
Iteration 143, loss = 0.44521351
Iteration 144, loss = 0.46425663
Iteration 145, loss = 0.44705811
Iteration 146, loss = 0.44692157
Iteration 147, loss = 0.44228444
Iteration 148, loss = 0.44114365
Iteration 149, loss = 0.43743366
Iteration 150, loss = 0.44019571
Iteration 151, loss = 0.43560949
Iteration 152, loss = 0.44167549
Iteration 153, loss = 0.43684731
Iteration 154, loss = 0.43446613
Iteration 155, loss = 0.43777207
Iteration 156, loss = 0.43303570
Iteration 157, loss = 0.43217190
Iteration 158, loss = 0.43261818
Iteration 159, loss = 0.43187021
Iteration 160, loss = 0.43604723
Iteration 161, loss = 0.43749712
Iteration 162, loss = 0.44239802
Iteration 163, loss = 0.43724533
Iteration 164, loss = 0.43288615
Iteration 165, loss = 0.43121529
Iteration 166, loss = 0.44132126
Iteration 167, loss = 0.43693849
Iteration 168, loss = 0.43378429
Iteration 169, loss = 0.43114752
Iteration 170, loss = 0.44363845
Iteration 171, loss = 0.43711202
Iteration 172, loss = 0.45568732
Iteration 173, loss = 0.43401544
Iteration 174, loss = 0.44248338
Iteration 175, loss = 0.43948424
Iteration 176, loss = 0.43169912
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 1/5] END ...........alpha=0.2, momentum=0.4;, score=0.852 total time=   5.7s
Iteration 1, loss = 1.12770923
Iteration 2, loss = 0.99461912
Iteration 3, loss = 0.94624024
Iteration 4, loss = 0.91280411
Iteration 5, loss = 0.89714279
Iteration 6, loss = 0.87368671
Iteration 7, loss = 0.85338693
Iteration 8, loss = 0.83803451
Iteration 9, loss = 0.82382520
Iteration 10, loss = 0.81540536
Iteration 11, loss = 0.81019009
Iteration 12, loss = 0.79888487
Iteration 13, loss = 0.81415816
Iteration 14, loss = 0.78222307
Iteration 15, loss = 0.77163224
Iteration 16, loss = 0.77131384
Iteration 17, loss = 0.75730911
Iteration 18, loss = 0.76308706
Iteration 19, loss = 0.75117848
Iteration 20, loss = 0.74608773
Iteration 21, loss = 0.73849780
Iteration 22, loss = 0.73326835
Iteration 23, loss = 0.73560358
Iteration 24, loss = 0.73405177
Iteration 25, loss = 0.72737011
Iteration 26, loss = 0.72905316
Iteration 27, loss = 0.72128744
Iteration 28, loss = 0.71560790
Iteration 29, loss = 0.72639403
Iteration 30, loss = 0.71288563
Iteration 31, loss = 0.70860879
Iteration 32, loss = 0.70231423
Iteration 33, loss = 0.69847037
Iteration 34, loss = 0.69869354
Iteration 35, loss = 0.69936554
Iteration 36, loss = 0.69327734
Iteration 37, loss = 0.68833099
Iteration 38, loss = 0.68532277
Iteration 39, loss = 0.68541464
Iteration 40, loss = 0.68079530
Iteration 41, loss = 0.68158510
Iteration 42, loss = 0.67926382
Iteration 43, loss = 0.67583869
Iteration 44, loss = 0.68819635
Iteration 45, loss = 0.68058809
Iteration 46, loss = 0.68313804
Iteration 47, loss = 0.67464299
Iteration 48, loss = 0.67144926
Iteration 49, loss = 0.67707833
Iteration 50, loss = 0.67899383
Iteration 51, loss = 0.67313338
Iteration 52, loss = 0.67988444
Iteration 53, loss = 0.66912752
Iteration 54, loss = 0.66210459
Iteration 55, loss = 0.65780341
Iteration 56, loss = 0.67173113
Iteration 57, loss = 0.65712331
Iteration 58, loss = 0.65971110
Iteration 59, loss = 0.65342740
Iteration 60, loss = 0.65188448
Iteration 61, loss = 0.65503148
Iteration 62, loss = 0.68175067
Iteration 63, loss = 0.65572968
Iteration 64, loss = 0.64906385
Iteration 65, loss = 0.66540609
Iteration 66, loss = 0.65258151Iteration 1, loss = 1.08224138
Iteration 2, loss = 0.94954289
Iteration 3, loss = 0.89203787
Iteration 4, loss = 0.85366816
Iteration 5, loss = 0.82141724
Iteration 6, loss = 0.79261012
Iteration 7, loss = 0.76536876
Iteration 8, loss = 0.74471011
Iteration 9, loss = 0.72281997
Iteration 10, loss = 0.70569619
Iteration 11, loss = 0.69957812
Iteration 12, loss = 0.67982215
Iteration 13, loss = 0.66391814
Iteration 14, loss = 0.65150865
Iteration 15, loss = 0.63833168
Iteration 16, loss = 0.62666128
Iteration 17, loss = 0.61839209
Iteration 18, loss = 0.61136817
Iteration 19, loss = 0.59773771
Iteration 20, loss = 0.60715288
Iteration 21, loss = 0.59053408
Iteration 22, loss = 0.57704383
Iteration 23, loss = 0.56801245
Iteration 24, loss = 0.56033888
Iteration 25, loss = 0.55667113
Iteration 26, loss = 0.54876780
Iteration 27, loss = 0.54278361
Iteration 28, loss = 0.53435284
Iteration 29, loss = 0.53229161
Iteration 30, loss = 0.52337410
Iteration 31, loss = 0.52740184
Iteration 32, loss = 0.51956769
Iteration 33, loss = 0.50960902
Iteration 34, loss = 0.50596739
Iteration 35, loss = 0.50029958
Iteration 36, loss = 0.49559663
Iteration 37, loss = 0.49532099
Iteration 38, loss = 0.48900213
Iteration 39, loss = 0.48446175
Iteration 40, loss = 0.48096784
Iteration 41, loss = 0.47477271
Iteration 42, loss = 0.47117344
Iteration 43, loss = 0.46775354
Iteration 44, loss = 0.46666702
Iteration 45, loss = 0.46256763
Iteration 46, loss = 0.45731063
Iteration 47, loss = 0.45812451
Iteration 48, loss = 0.45030181
Iteration 49, loss = 0.45167726
Iteration 50, loss = 0.44503873
Iteration 51, loss = 0.44045455
Iteration 52, loss = 0.44102885
Iteration 53, loss = 0.43554258
Iteration 54, loss = 0.44223550
Iteration 55, loss = 0.43305023
Iteration 56, loss = 0.43042820
Iteration 57, loss = 0.43480236
Iteration 58, loss = 0.42942489
Iteration 59, loss = 0.42222432
Iteration 60, loss = 0.42576288
Iteration 61, loss = 0.42069764
Iteration 62, loss = 0.42027627
Iteration 63, loss = 0.42181977
Iteration 64, loss = 0.41254530
Iteration 65, loss = 0.40949151
Iteration 66, loss = 0.40775957
Iteration 67, loss = 0.40594411
Iteration 68, loss = 0.40368239
Iteration 69, loss = 0.40695936
Iteration 70, loss = 0.40375558
Iteration 71, loss = 0.40277219
Iteration 72, loss = 0.39991473
Iteration 73, loss = 0.40774346
Iteration 74, loss = 0.39791161
Iteration 75, loss = 0.39399052
Iteration 76, loss = 0.39173157
Iteration 77, loss = 0.39109521
Iteration 78, loss = 0.40134742
Iteration 79, loss = 0.39991170
Iteration 80, loss = 0.39145587
Iteration 81, loss = 0.39076055
Iteration 82, loss = 0.39024515
Iteration 83, loss = 0.38483474
Iteration 84, loss = 0.38440346
Iteration 85, loss = 0.38173777
Iteration 86, loss = 0.38537420
Iteration 87, loss = 0.38366874
Iteration 88, loss = 0.38432218
Iteration 89, loss = 0.38119990
Iteration 90, loss = 0.38004870
Iteration 91, loss = 0.37690103
Iteration 92, loss = 0.37692862
Iteration 93, loss = 0.37760563
Iteration 94, loss = 0.37442428
Iteration 95, loss = 0.37387661
Iteration 96, loss = 0.37150044
Iteration 97, loss = 0.37042170
Iteration 98, loss = 0.38011294
Iteration 99, loss = 0.37011300
Iteration 100, loss = 0.37164884
Iteration 101, loss = 0.37043210
Iteration 102, loss = 0.39070808
Iteration 103, loss = 0.37893448
Iteration 104, loss = 0.37489747
Iteration 105, loss = 0.36657280
Iteration 106, loss = 0.36444452
Iteration 107, loss = 0.36361372
Iteration 108, loss = 0.36853639
Iteration 109, loss = 0.36453888
Iteration 110, loss = 0.36104213
Iteration 111, loss = 0.36404800
Iteration 112, loss = 0.36562708
Iteration 113, loss = 0.36079497
Iteration 114, loss = 0.36201245
Iteration 115, loss = 0.36589786
Iteration 116, loss = 0.36352701
Iteration 117, loss = 0.36996469
Iteration 118, loss = 0.36822514
Iteration 119, loss = 0.36319921
Iteration 120, loss = 0.36275605
Iteration 121, loss = 0.35919121
Iteration 122, loss = 0.35663755
Iteration 123, loss = 0.35482715
Iteration 124, loss = 0.35794086
Iteration 125, loss = 0.35484749
Iteration 126, loss = 0.35775979
Iteration 127, loss = 0.35499300
Iteration 128, loss = 0.35693746
Iteration 129, loss = 0.35192540
Iteration 130, loss = 0.35171370
Iteration 131, loss = 0.35198805
Iteration 132, loss = 0.35327562
Iteration 133, loss = 0.35067786
Iteration 134, loss = 0.35217508
Iteration 135, loss = 0.35450696
Iteration 136, loss = 0.35017187
Iteration 137, loss = 0.34844723
Iteration 138, loss = 0.35249522
Iteration 139, loss = 0.35571533
Iteration 140, loss = 0.35055585
Iteration 141, loss = 0.35432992
Iteration 142, loss = 0.35468408
Iteration 143, loss = 0.35713559
Iteration 144, loss = 0.35520865
Iteration 145, loss = 0.35160090
Iteration 146, loss = 0.34916274
Iteration 147, loss = 0.35905553
Iteration 148, loss = 0.37801495
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 4/5] END ...........alpha=0.2, momentum=0.4;, score=0.879 total time=   4.8s
Iteration 1, loss = 1.07858086
Iteration 2, loss = 0.94345563
Iteration 3, loss = 0.88688446
Iteration 4, loss = 0.85098507
Iteration 5, loss = 0.81660604
Iteration 6, loss = 0.78885083
Iteration 7, loss = 0.76288851
Iteration 8, loss = 0.74178071
Iteration 9, loss = 0.72085758
Iteration 10, loss = 0.70117866
Iteration 11, loss = 0.69399247
Iteration 12, loss = 0.67853462
Iteration 13, loss = 0.65943754
Iteration 14, loss = 0.64893870
Iteration 15, loss = 0.63408910
Iteration 16, loss = 0.62158076
Iteration 17, loss = 0.61306269
Iteration 18, loss = 0.60276518
Iteration 19, loss = 0.59175949
Iteration 20, loss = 0.58638017
Iteration 21, loss = 0.57758275
Iteration 22, loss = 0.56815062
Iteration 23, loss = 0.56450178
Iteration 24, loss = 0.55495938
Iteration 25, loss = 0.55033344
Iteration 26, loss = 0.54125789
Iteration 27, loss = 0.53571826
Iteration 28, loss = 0.52751509
Iteration 29, loss = 0.52562748
Iteration 30, loss = 0.51627335
Iteration 31, loss = 0.51576777
Iteration 32, loss = 0.51052651
Iteration 33, loss = 0.50214853
Iteration 34, loss = 0.49617258
Iteration 35, loss = 0.49042322
Iteration 36, loss = 0.48759641
Iteration 37, loss = 0.48607683
Iteration 38, loss = 0.48126429
Iteration 39, loss = 0.47582628
Iteration 40, loss = 0.47161926
Iteration 41, loss = 0.46690511
Iteration 42, loss = 0.46301011
Iteration 43, loss = 0.45836715
Iteration 44, loss = 0.47016230
Iteration 45, loss = 0.45564757
Iteration 46, loss = 0.44891832
Iteration 47, loss = 0.44931815
Iteration 48, loss = 0.44259909
Iteration 49, loss = 0.44444605
Iteration 50, loss = 0.43902879
Iteration 51, loss = 0.43536917
Iteration 52, loss = 0.43328216
Iteration 53, loss = 0.42745241
Iteration 54, loss = 0.43822353
Iteration 55, loss = 0.42689830
Iteration 56, loss = 0.42341649
Iteration 57, loss = 0.42657271
Iteration 58, loss = 0.42316720
Iteration 59, loss = 0.41541893
Iteration 60, loss = 0.42107130
Iteration 61, loss = 0.41239181
Iteration 62, loss = 0.41208274
Iteration 63, loss = 0.40937964
Iteration 64, loss = 0.40440120
Iteration 65, loss = 0.40151807
Iteration 66, loss = 0.39974491
Iteration 67, loss = 0.39788384
Iteration 68, loss = 0.39651500
Iteration 69, loss = 0.40011788
Iteration 70, loss = 0.39685760
Iteration 71, loss = 0.39513349
Iteration 72, loss = 0.39210613
Iteration 73, loss = 0.39942326
Iteration 74, loss = 0.39203235
Iteration 75, loss = 0.38664350
Iteration 76, loss = 0.38409136
Iteration 77, loss = 0.38808770
Iteration 78, loss = 0.40154297
Iteration 79, loss = 0.39839807
Iteration 80, loss = 0.38363491
Iteration 81, loss = 0.38292189
Iteration 82, loss = 0.38355528
Iteration 83, loss = 0.37799485
Iteration 84, loss = 0.37720376
Iteration 85, loss = 0.37568688
Iteration 86, loss = 0.38053605
Iteration 87, loss = 0.37962764
Iteration 88, loss = 0.37391558
Iteration 89, loss = 0.37081006
Iteration 90, loss = 0.37263026
Iteration 91, loss = 0.37329346
Iteration 92, loss = 0.38331648
Iteration 93, loss = 0.37410572
Iteration 94, loss = 0.36904768
Iteration 95, loss = 0.37015045
Iteration 55, loss = 0.66118665
Iteration 56, loss = 0.66817153
Iteration 57, loss = 0.65646534
Iteration 58, loss = 0.65780183
Iteration 59, loss = 0.65250321
Iteration 60, loss = 0.65197550
Iteration 61, loss = 0.65299339
Iteration 62, loss = 0.68661962
Iteration 63, loss = 0.65716429
Iteration 64, loss = 0.66199260
Iteration 65, loss = 0.66078098
Iteration 66, loss = 0.65102375
Iteration 67, loss = 0.65136964
Iteration 68, loss = 0.64657018
Iteration 69, loss = 0.64558607
Iteration 70, loss = 0.65423606
Iteration 71, loss = 0.64288380
Iteration 72, loss = 0.64258480
Iteration 73, loss = 0.64379233
Iteration 74, loss = 0.65519398
Iteration 75, loss = 0.65067918
Iteration 76, loss = 0.64179708
Iteration 77, loss = 0.63725669
Iteration 78, loss = 0.64197169
Iteration 79, loss = 0.63549527
Iteration 80, loss = 0.63259660
Iteration 81, loss = 0.63379119
Iteration 82, loss = 0.63392843
Iteration 83, loss = 0.63257563
Iteration 84, loss = 0.62883161
Iteration 85, loss = 0.62799391
Iteration 86, loss = 0.62920388
Iteration 87, loss = 0.63420407
Iteration 88, loss = 0.62828215
Iteration 89, loss = 0.62499914
Iteration 90, loss = 0.62934310
Iteration 91, loss = 0.63721529
Iteration 92, loss = 0.62664499
Iteration 93, loss = 0.62375167
Iteration 94, loss = 0.63003192
Iteration 95, loss = 0.63695634
Iteration 96, loss = 0.62408071
Iteration 97, loss = 0.62075695
Iteration 98, loss = 0.62365591
Iteration 99, loss = 0.65758633
Iteration 100, loss = 0.63080797
Iteration 101, loss = 0.62844721
Iteration 102, loss = 0.62962010
Iteration 103, loss = 0.62435196
Iteration 104, loss = 0.62445112
Iteration 105, loss = 0.62092452
Iteration 106, loss = 0.62405357
Iteration 107, loss = 0.62122277
Iteration 108, loss = 0.62463418
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 2/5] END ...........alpha=0.4, momentum=0.2;, score=0.773 total time=   3.3s
Iteration 1, loss = 1.12324751
Iteration 2, loss = 0.98909959
Iteration 3, loss = 0.93425707
Iteration 4, loss = 0.89854507
Iteration 5, loss = 0.86900998
Iteration 6, loss = 0.84292550
Iteration 7, loss = 0.81816396
Iteration 8, loss = 0.79987931
Iteration 9, loss = 0.78080693
Iteration 10, loss = 0.76628460
Iteration 11, loss = 0.76231632
Iteration 12, loss = 0.74407426
Iteration 13, loss = 0.73057408
Iteration 14, loss = 0.72076896
Iteration 15, loss = 0.70909895
Iteration 16, loss = 0.69924873
Iteration 17, loss = 0.69229876
Iteration 18, loss = 0.68824146
Iteration 19, loss = 0.67560314
Iteration 20, loss = 0.68410409
Iteration 21, loss = 0.67226171
Iteration 22, loss = 0.65976095
Iteration 23, loss = 0.65198283
Iteration 24, loss = 0.64579197
Iteration 25, loss = 0.64272815
Iteration 26, loss = 0.63659457
Iteration 27, loss = 0.63234446
Iteration 28, loss = 0.62543036
Iteration 29, loss = 0.62356266
Iteration 30, loss = 0.61630623
Iteration 31, loss = 0.62087515
Iteration 32, loss = 0.61469081
Iteration 33, loss = 0.60568354
Iteration 34, loss = 0.60338750
Iteration 35, loss = 0.59858927
Iteration 36, loss = 0.59612434
Iteration 37, loss = 0.59488655
Iteration 38, loss = 0.59031640
Iteration 39, loss = 0.58584205
Iteration 40, loss = 0.58427296
Iteration 41, loss = 0.57850750
Iteration 42, loss = 0.57620496
Iteration 43, loss = 0.57377536
Iteration 44, loss = 0.57324775
Iteration 45, loss = 0.56930370
Iteration 46, loss = 0.56478966
Iteration 47, loss = 0.56590278
Iteration 48, loss = 0.55897381
Iteration 49, loss = 0.56057658
Iteration 50, loss = 0.55479200
Iteration 51, loss = 0.55186253
Iteration 52, loss = 0.55267664
Iteration 53, loss = 0.54665467
Iteration 54, loss = 0.55437373
Iteration 55, loss = 0.54655791
Iteration 56, loss = 0.54545511
Iteration 57, loss = 0.54769346
Iteration 58, loss = 0.54247155
Iteration 59, loss = 0.53678216
Iteration 60, loss = 0.54030050
Iteration 61, loss = 0.53615962
Iteration 62, loss = 0.53635838
Iteration 63, loss = 0.54021322
Iteration 64, loss = 0.53007395
Iteration 65, loss = 0.52750525
Iteration 66, loss = 0.52641475
Iteration 67, loss = 0.52538742
Iteration 68, loss = 0.52258517
Iteration 69, loss = 0.52785787
Iteration 70, loss = 0.52461501
Iteration 71, loss = 0.52266141
Iteration 72, loss = 0.52069987
Iteration 73, loss = 0.52710034
Iteration 74, loss = 0.51922516
Iteration 75, loss = 0.51576758
Iteration 76, loss = 0.51351306
Iteration 77, loss = 0.51385857
Iteration 78, loss = 0.51934796
Iteration 79, loss = 0.52540482
Iteration 80, loss = 0.51533589
Iteration 81, loss = 0.51516179
Iteration 82, loss = 0.51575568
Iteration 83, loss = 0.51022611
Iteration 84, loss = 0.51123741
Iteration 85, loss = 0.50691482
Iteration 86, loss = 0.51019628
Iteration 87, loss = 0.50828605
Iteration 88, loss = 0.50732084
Iteration 89, loss = 0.50809979
Iteration 90, loss = 0.51032222
Iteration 91, loss = 0.50556219
Iteration 92, loss = 0.50390346
Iteration 93, loss = 0.50382084
Iteration 94, loss = 0.50154196
Iteration 95, loss = 0.50243844
Iteration 96, loss = 0.49989286
Iteration 97, loss = 0.49850085
Iteration 98, loss = 0.50384460
Iteration 99, loss = 0.49852323
Iteration 100, loss = 0.49931615
Iteration 101, loss = 0.50188472
Iteration 102, loss = 0.51707142
Iteration 103, loss = 0.50687550
Iteration 104, loss = 0.50217177
Iteration 105, loss = 0.49539201
Iteration 106, loss = 0.49407885
Iteration 107, loss = 0.49332645
Iteration 108, loss = 0.49779954
Iteration 109, loss = 0.49354238
Iteration 110, loss = 0.49021773
Iteration 111, loss = 0.49380209
Iteration 112, loss = 0.49615238
Iteration 113, loss = 0.49193612
Iteration 114, loss = 0.49133461
Iteration 115, loss = 0.49597658
Iteration 116, loss = 0.49426117
Iteration 117, loss = 0.49904556
Iteration 118, loss = 0.50029833
Iteration 119, loss = 0.49373277
Iteration 120, loss = 0.49216629
Iteration 121, loss = 0.49103685
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 4/5] END ...........alpha=0.4, momentum=0.2;, score=0.850 total time=   4.2s
Iteration 1, loss = 1.06135946
Iteration 2, loss = 0.92554872
Iteration 3, loss = 0.86429493
Iteration 4, loss = 0.82432790
Iteration 5, loss = 0.79010093
Iteration 6, loss = 0.75899313
Iteration 7, loss = 0.73092506
Iteration 8, loss = 0.70744367
Iteration 9, loss = 0.68378128
Iteration 10, loss = 0.66851749
Iteration 11, loss = 0.65799313
Iteration 12, loss = 0.63726852
Iteration 13, loss = 0.61842809
Iteration 14, loss = 0.60547340
Iteration 15, loss = 0.58988450
Iteration 16, loss = 0.57631131
Iteration 17, loss = 0.56521394
Iteration 18, loss = 0.55664582
Iteration 19, loss = 0.54331954
Iteration 20, loss = 0.55291620
Iteration 21, loss = 0.53276860
Iteration 22, loss = 0.51780056
Iteration 23, loss = 0.50867836
Iteration 24, loss = 0.50015311
Iteration 25, loss = 0.49491089
Iteration 26, loss = 0.48573425
Iteration 27, loss = 0.47788951
Iteration 28, loss = 0.47785626
Iteration 29, loss = 0.46854775
Iteration 30, loss = 0.45955513
Iteration 31, loss = 0.45693885
Iteration 32, loss = 0.45156989
Iteration 33, loss = 0.44136507
Iteration 34, loss = 0.43760020
Iteration 35, loss = 0.43246112
Iteration 36, loss = 0.42661859
Iteration 37, loss = 0.42670530
Iteration 38, loss = 0.41908224
Iteration 39, loss = 0.41702174
Iteration 40, loss = 0.40929411
Iteration 41, loss = 0.40533333
Iteration 42, loss = 0.39998284
Iteration 43, loss = 0.39556465
Iteration 44, loss = 0.39458241
Iteration 45, loss = 0.39158586
Iteration 46, loss = 0.38502378
Iteration 47, loss = 0.38493895
Iteration 48, loss = 0.37821082
Iteration 49, loss = 0.37466463
Iteration 50, loss = 0.37134009
Iteration 51, loss = 0.36707894
Iteration 52, loss = 0.36421868
Iteration 53, loss = 0.36241982
Iteration 54, loss = 0.36216445
Iteration 55, loss = 0.35626179
Iteration 56, loss = 0.35334481
Iteration 57, loss = 0.35833248
Iteration 58, loss = 0.35531025
Iteration 59, loss = 0.34710058
Iteration 60, loss = 0.34857107
Iteration 61, loss = 0.34452262
Iteration 62, loss = 0.33783376
Iteration 63, loss = 0.34722312
Iteration 96, loss = 0.36833640
Iteration 97, loss = 0.36469285
Iteration 98, loss = 0.37199247
Iteration 99, loss = 0.36492490
Iteration 100, loss = 0.36509164
Iteration 101, loss = 0.36422755
Iteration 102, loss = 0.37704987
Iteration 103, loss = 0.37069765
Iteration 104, loss = 0.36963825
Iteration 105, loss = 0.36174459
Iteration 106, loss = 0.35917724
Iteration 107, loss = 0.35749436
Iteration 108, loss = 0.36112707
Iteration 109, loss = 0.35700325
Iteration 110, loss = 0.35507389
Iteration 111, loss = 0.35785145
Iteration 112, loss = 0.35622161
Iteration 113, loss = 0.35629154
Iteration 114, loss = 0.35406237
Iteration 115, loss = 0.35717304
Iteration 116, loss = 0.35739466
Iteration 117, loss = 0.36362249
Iteration 118, loss = 0.37381999
Iteration 119, loss = 0.35860655
Iteration 120, loss = 0.35572925
Iteration 121, loss = 0.35302426
Iteration 122, loss = 0.35340450
Iteration 123, loss = 0.35462375
Iteration 124, loss = 0.35823898
Iteration 125, loss = 0.35126430
Iteration 126, loss = 0.35266626
Iteration 127, loss = 0.35553795
Iteration 128, loss = 0.35265350
Iteration 129, loss = 0.34733697
Iteration 130, loss = 0.34647624
Iteration 131, loss = 0.34628543
Iteration 132, loss = 0.34806855
Iteration 133, loss = 0.34517264
Iteration 134, loss = 0.34526705
Iteration 135, loss = 0.34685529
Iteration 136, loss = 0.34470515
Iteration 137, loss = 0.34448243
Iteration 138, loss = 0.34789764
Iteration 139, loss = 0.34970944
Iteration 140, loss = 0.34471357
Iteration 141, loss = 0.34670156
Iteration 142, loss = 0.34915196
Iteration 143, loss = 0.35244703
Iteration 144, loss = 0.35175347
Iteration 145, loss = 0.34716636
Iteration 146, loss = 0.34470945
Iteration 147, loss = 0.34998896
Iteration 148, loss = 0.35860677
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 5/5] END ...........alpha=0.2, momentum=0.4;, score=0.879 total time=   4.6s
Iteration 1, loss = 1.11957161
Iteration 2, loss = 0.98356300
Iteration 3, loss = 0.92990322
Iteration 4, loss = 0.89661828
Iteration 5, loss = 0.86525879
Iteration 6, loss = 0.84033779
Iteration 7, loss = 0.81704117
Iteration 8, loss = 0.79874333
Iteration 9, loss = 0.78073893
Iteration 10, loss = 0.76378556
Iteration 11, loss = 0.75871498
Iteration 12, loss = 0.74467010
Iteration 13, loss = 0.72851135
Iteration 14, loss = 0.72037468
Iteration 15, loss = 0.70745849
Iteration 16, loss = 0.69674051
Iteration 17, loss = 0.69007429
Iteration 18, loss = 0.68216208
Iteration 19, loss = 0.67224179
Iteration 20, loss = 0.66767945
Iteration 21, loss = 0.66073799
Iteration 22, loss = 0.65334762
Iteration 23, loss = 0.65059948
Iteration 24, loss = 0.64270267
Iteration 25, loss = 0.63895621
Iteration 26, loss = 0.63159648
Iteration 27, loss = 0.62800407
Iteration 28, loss = 0.62117250
Iteration 29, loss = 0.61913812
Iteration 30, loss = 0.61226109
Iteration 31, loss = 0.61267480
Iteration 32, loss = 0.60831146
Iteration 33, loss = 0.60096978
Iteration 34, loss = 0.59637667
Iteration 35, loss = 0.59117659
Iteration 36, loss = 0.59082032
Iteration 37, loss = 0.58724391
Iteration 38, loss = 0.58456369
Iteration 39, loss = 0.57964720
Iteration 40, loss = 0.57801626
Iteration 41, loss = 0.57335094
Iteration 42, loss = 0.57105815
Iteration 43, loss = 0.56653822
Iteration 44, loss = 0.57546432
Iteration 45, loss = 0.56440305
Iteration 46, loss = 0.55857335
Iteration 47, loss = 0.55919125
Iteration 48, loss = 0.55377136
Iteration 49, loss = 0.55425461
Iteration 50, loss = 0.55176624
Iteration 51, loss = 0.54779199
Iteration 52, loss = 0.54833832
Iteration 53, loss = 0.54135516
Iteration 54, loss = 0.55077079
Iteration 55, loss = 0.54238049
Iteration 56, loss = 0.53959895
Iteration 57, loss = 0.54058466
Iteration 58, loss = 0.53848245
Iteration 59, loss = 0.53116808
Iteration 60, loss = 0.53730053
Iteration 61, loss = 0.52960187
Iteration 62, loss = 0.52963553
Iteration 63, loss = 0.52980200
Iteration 64, loss = 0.52397322
Iteration 65, loss = 0.52126160
Iteration 66, loss = 0.51971334
Iteration 67, loss = 0.51800929
Iteration 68, loss = 0.51710085
Iteration 69, loss = 0.52318290
Iteration 70, loss = 0.51936864
Iteration 71, loss = 0.51651184
Iteration 72, loss = 0.51456784
Iteration 73, loss = 0.52318969
Iteration 74, loss = 0.51558549
Iteration 75, loss = 0.51030792
Iteration 76, loss = 0.50737320
Iteration 77, loss = 0.50940315
Iteration 78, loss = 0.51909224
Iteration 79, loss = 0.52053492
Iteration 80, loss = 0.50678360
Iteration 81, loss = 0.50771993
Iteration 82, loss = 0.50881443
Iteration 83, loss = 0.50369864
Iteration 84, loss = 0.50389402
Iteration 85, loss = 0.50175016
Iteration 86, loss = 0.50630792
Iteration 87, loss = 0.50716446
Iteration 88, loss = 0.50028643
Iteration 89, loss = 0.49731938
Iteration 90, loss = 0.50624545
Iteration 91, loss = 0.50520994
Iteration 92, loss = 0.50903279
Iteration 93, loss = 0.50024277
Iteration 94, loss = 0.49629188
Iteration 95, loss = 0.49990208
Iteration 96, loss = 0.49867176
Iteration 97, loss = 0.49393882
Iteration 98, loss = 0.49743090
Iteration 99, loss = 0.49425743
Iteration 100, loss = 0.49405191
Iteration 101, loss = 0.49753338
Iteration 102, loss = 0.50863058
Iteration 103, loss = 0.50169984
Iteration 104, loss = 0.49710413
Iteration 105, loss = 0.49132083
Iteration 106, loss = 0.48917500
Iteration 107, loss = 0.48755657
Iteration 108, loss = 0.49037481
Iteration 109, loss = 0.48703487
Iteration 110, loss = 0.48479148
Iteration 111, loss = 0.48653878
Iteration 112, loss = 0.48710533
Iteration 113, loss = 0.48723213
Iteration 114, loss = 0.48427949
Iteration 115, loss = 0.48836026
Iteration 116, loss = 0.48713023
Iteration 117, loss = 0.49324626
Iteration 118, loss = 0.50387915
Iteration 119, loss = 0.48875341
Iteration 120, loss = 0.48795041
Iteration 121, loss = 0.48659743
Iteration 122, loss = 0.48662554
Iteration 123, loss = 0.48790173
Iteration 124, loss = 0.48720916
Iteration 125, loss = 0.48322936
Iteration 126, loss = 0.48293515
Iteration 127, loss = 0.48457491
Iteration 128, loss = 0.48636258
Iteration 129, loss = 0.47973474
Iteration 130, loss = 0.47906185
Iteration 131, loss = 0.48020996
Iteration 132, loss = 0.48225734
Iteration 133, loss = 0.47837487
Iteration 134, loss = 0.47796106
Iteration 135, loss = 0.48206474
Iteration 136, loss = 0.47760851
Iteration 137, loss = 0.47824955
Iteration 138, loss = 0.48292608
Iteration 139, loss = 0.47944858
Iteration 140, loss = 0.47668459
Iteration 141, loss = 0.48021114
Iteration 142, loss = 0.48052498
Iteration 143, loss = 0.48131448
Iteration 144, loss = 0.48584058
Iteration 145, loss = 0.48074085
Iteration 146, loss = 0.48202083
Iteration 147, loss = 0.48835771
Iteration 148, loss = 0.49983816
Iteration 149, loss = 0.48345607
Iteration 150, loss = 0.47704557
Iteration 151, loss = 0.47472325
Iteration 152, loss = 0.47890776
Iteration 153, loss = 0.47839523
Iteration 154, loss = 0.47600553
Iteration 155, loss = 0.47608628
Iteration 156, loss = 0.47372028
Iteration 157, loss = 0.47597999
Iteration 158, loss = 0.47532997
Iteration 159, loss = 0.47620860
Iteration 160, loss = 0.47320232
Iteration 161, loss = 0.48659489
Iteration 162, loss = 0.47626869
Iteration 163, loss = 0.47458551
Iteration 164, loss = 0.47235022
Iteration 165, loss = 0.47302785
Iteration 166, loss = 0.47421849
Iteration 167, loss = 0.48476834
Iteration 168, loss = 0.47842204
Iteration 169, loss = 0.47361739
Iteration 170, loss = 0.47444780
Iteration 171, loss = 0.47729803
Iteration 172, loss = 0.47371904
Iteration 173, loss = 0.47604574
Iteration 174, loss = 0.47469789
Iteration 175, loss = 0.47236750
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 5/5] END ...........alpha=0.4, momentum=0.2;, score=0.846 total time=   6.6s
Iteration 1, loss = 1.06111586
Iteration 2, loss = 0.92769786
Iteration 3, loss = 0.86734774
Iteration 4, loss = 0.82582550
Iteration 5, loss = 0.79032132
Iteration 6, loss = 0.75897886
Iteration 7, loss = 0.72905562
Iteration 40, loss = 0.58588956
Iteration 41, loss = 0.58159190
Iteration 42, loss = 0.57944043
Iteration 43, loss = 0.57572128
Iteration 44, loss = 0.57543398
Iteration 45, loss = 0.57423642
Iteration 46, loss = 0.56731406
Iteration 47, loss = 0.56845975
Iteration 48, loss = 0.56204450
Iteration 49, loss = 0.56022540
Iteration 50, loss = 0.55816619
Iteration 51, loss = 0.55418739
Iteration 52, loss = 0.55101395
Iteration 53, loss = 0.54942881
Iteration 54, loss = 0.55248598
Iteration 55, loss = 0.54578742
Iteration 56, loss = 0.54319298
Iteration 57, loss = 0.54791283
Iteration 58, loss = 0.54475885
Iteration 59, loss = 0.53973372
Iteration 60, loss = 0.54398542
Iteration 61, loss = 0.53915160
Iteration 62, loss = 0.53280499
Iteration 63, loss = 0.54357862
Iteration 64, loss = 0.53209807
Iteration 65, loss = 0.52860520
Iteration 66, loss = 0.53053623
Iteration 67, loss = 0.52552595
Iteration 68, loss = 0.52373213
Iteration 69, loss = 0.52528835
Iteration 70, loss = 0.52483951
Iteration 71, loss = 0.52149743
Iteration 72, loss = 0.52045051
Iteration 73, loss = 0.51908878
Iteration 74, loss = 0.51735469
Iteration 75, loss = 0.51642545
Iteration 76, loss = 0.51480734
Iteration 77, loss = 0.51474536
Iteration 78, loss = 0.52357337
Iteration 79, loss = 0.51540048
Iteration 80, loss = 0.51430353
Iteration 81, loss = 0.51306660
Iteration 82, loss = 0.51404726
Iteration 83, loss = 0.50964646
Iteration 84, loss = 0.51210402
Iteration 85, loss = 0.51191177
Iteration 86, loss = 0.50806622
Iteration 87, loss = 0.50598260
Iteration 88, loss = 0.50858397
Iteration 89, loss = 0.50810186
Iteration 90, loss = 0.51225285
Iteration 91, loss = 0.50478311
Iteration 92, loss = 0.50359659
Iteration 93, loss = 0.50312920
Iteration 94, loss = 0.50064514
Iteration 95, loss = 0.50115083
Iteration 96, loss = 0.49824335
Iteration 97, loss = 0.49692733
Iteration 98, loss = 0.50605459
Iteration 99, loss = 0.50044728
Iteration 100, loss = 0.49862776
Iteration 101, loss = 0.50086388
Iteration 102, loss = 0.49979855
Iteration 103, loss = 0.50095744
Iteration 104, loss = 0.50360387
Iteration 105, loss = 0.49737717
Iteration 106, loss = 0.49187761
Iteration 107, loss = 0.49111033
Iteration 108, loss = 0.50261232
Iteration 109, loss = 0.49200463
Iteration 110, loss = 0.49079075
Iteration 111, loss = 0.48899911
Iteration 112, loss = 0.49363735
Iteration 113, loss = 0.48991535
Iteration 114, loss = 0.48837283
Iteration 115, loss = 0.49187135
Iteration 116, loss = 0.49278503
Iteration 117, loss = 0.48863019
Iteration 118, loss = 0.49388543
Iteration 119, loss = 0.49070147
Iteration 120, loss = 0.49019172
Iteration 121, loss = 0.49182022
Iteration 122, loss = 0.48730536
Iteration 123, loss = 0.48634304
Iteration 124, loss = 0.48626566
Iteration 125, loss = 0.48491555
Iteration 126, loss = 0.48727654
Iteration 127, loss = 0.48483532
Iteration 128, loss = 0.48567179
Iteration 129, loss = 0.48183416
Iteration 130, loss = 0.48148782
Iteration 131, loss = 0.48554379
Iteration 132, loss = 0.48657807
Iteration 133, loss = 0.48221056
Iteration 134, loss = 0.48290676
Iteration 135, loss = 0.48154081
Iteration 136, loss = 0.47987513
Iteration 137, loss = 0.47915931
Iteration 138, loss = 0.48382806
Iteration 139, loss = 0.48328962
Iteration 140, loss = 0.48051373
Iteration 141, loss = 0.50506573
Iteration 142, loss = 0.48774827
Iteration 143, loss = 0.48302122
Iteration 144, loss = 0.49241991
Iteration 145, loss = 0.48394678
Iteration 146, loss = 0.49165486
Iteration 147, loss = 0.47999763
Iteration 148, loss = 0.49584000
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 3/5] END ...........alpha=0.4, momentum=0.2;, score=0.832 total time=   4.5s
Iteration 1, loss = 1.06557656
Iteration 2, loss = 0.92981428
Iteration 3, loss = 0.87057829
Iteration 4, loss = 0.82802732
Iteration 5, loss = 0.80535843
Iteration 6, loss = 0.77634615
Iteration 7, loss = 0.75055404
Iteration 8, loss = 0.72999396
Iteration 9, loss = 0.71049473
Iteration 10, loss = 0.69761299
Iteration 11, loss = 0.68448534
Iteration 12, loss = 0.66955277
Iteration 13, loss = 0.67667464
Iteration 14, loss = 0.64421130
Iteration 15, loss = 0.63126908
Iteration 16, loss = 0.62388919
Iteration 17, loss = 0.60862928
Iteration 18, loss = 0.60723048
Iteration 19, loss = 0.59320485
Iteration 20, loss = 0.58508912
Iteration 21, loss = 0.57343131
Iteration 22, loss = 0.56660342
Iteration 23, loss = 0.57044669
Iteration 24, loss = 0.56738281
Iteration 25, loss = 0.55471414
Iteration 26, loss = 0.55154271
Iteration 27, loss = 0.53984845
Iteration 28, loss = 0.53101016
Iteration 29, loss = 0.53915277
Iteration 30, loss = 0.52282316
Iteration 31, loss = 0.51743677
Iteration 32, loss = 0.51046541
Iteration 33, loss = 0.50402609
Iteration 34, loss = 0.50086042
Iteration 35, loss = 0.49920512
Iteration 36, loss = 0.49149384
Iteration 37, loss = 0.48579814
Iteration 38, loss = 0.48089389
Iteration 39, loss = 0.47777137
Iteration 40, loss = 0.47296667
Iteration 41, loss = 0.47574958
Iteration 42, loss = 0.46866301
Iteration 43, loss = 0.46303135
Iteration 44, loss = 0.47588997
Iteration 45, loss = 0.47524361
Iteration 46, loss = 0.45884030
Iteration 47, loss = 0.45570012
Iteration 48, loss = 0.45387681
Iteration 49, loss = 0.46071678
Iteration 50, loss = 0.44990972
Iteration 51, loss = 0.44474454
Iteration 52, loss = 0.45434777
Iteration 53, loss = 0.44023387
Iteration 54, loss = 0.43203911
Iteration 55, loss = 0.42750285
Iteration 56, loss = 0.43811062
Iteration 57, loss = 0.42535339
Iteration 58, loss = 0.42711137
Iteration 59, loss = 0.41805685
Iteration 60, loss = 0.41555694
Iteration 61, loss = 0.41325881
Iteration 62, loss = 0.45901136
Iteration 63, loss = 0.41816171
Iteration 64, loss = 0.40874130
Iteration 65, loss = 0.43744973
Iteration 66, loss = 0.41622205
Iteration 67, loss = 0.41132453
Iteration 68, loss = 0.40460672
Iteration 69, loss = 0.40340473
Iteration 70, loss = 0.41040555
Iteration 71, loss = 0.39804544
Iteration 72, loss = 0.39543594
Iteration 73, loss = 0.39788474
Iteration 74, loss = 0.39080419
Iteration 75, loss = 0.38906814
Iteration 76, loss = 0.38612240
Iteration 77, loss = 0.38499061
Iteration 78, loss = 0.38413875
Iteration 79, loss = 0.38651144
Iteration 80, loss = 0.38145621
Iteration 81, loss = 0.38273634
Iteration 82, loss = 0.37933207
Iteration 83, loss = 0.39206303
Iteration 84, loss = 0.38176194
Iteration 85, loss = 0.37531252
Iteration 86, loss = 0.37896224
Iteration 87, loss = 0.38132914
Iteration 88, loss = 0.37238927
Iteration 89, loss = 0.36792524
Iteration 90, loss = 0.36813980
Iteration 91, loss = 0.36651928
Iteration 92, loss = 0.36390950
Iteration 93, loss = 0.36236819
Iteration 94, loss = 0.36040944
Iteration 95, loss = 0.38445164
Iteration 96, loss = 0.36384615
Iteration 97, loss = 0.35890203
Iteration 98, loss = 0.36068144
Iteration 99, loss = 0.40268990
Iteration 100, loss = 0.37183845
Iteration 101, loss = 0.36540515
Iteration 102, loss = 0.36444516
Iteration 103, loss = 0.35760736
Iteration 104, loss = 0.35336336
Iteration 105, loss = 0.35103457
Iteration 106, loss = 0.35143860
Iteration 107, loss = 0.35485328
Iteration 108, loss = 0.35170432
Iteration 109, loss = 0.34873204
Iteration 110, loss = 0.34544066
Iteration 111, loss = 0.35042307
Iteration 112, loss = 0.34521055
Iteration 113, loss = 0.34892722
Iteration 114, loss = 0.34343174
Iteration 115, loss = 0.34191859
Iteration 116, loss = 0.34123962
Iteration 117, loss = 0.33915849
Iteration 118, loss = 0.33997660
Iteration 119, loss = 0.35017962
Iteration 120, loss = 0.35052307
Iteration 121, loss = 0.34109590
Iteration 122, loss = 0.35946123
Iteration 123, loss = 0.34633819
Iteration 124, loss = 0.33850497
Iteration 125, loss = 0.34759118
Iteration 126, loss = 0.33684497
Iteration 127, loss = 0.33322680
Iteration 128, loss = 0.33930352
Iteration 129, loss = 0.34092537
Iteration 130, loss = 0.33485813
Iteration 131, loss = 0.33166478
Iteration 132, loss = 0.33215627
Iteration 64, loss = 0.33482599
Iteration 65, loss = 0.33092711
Iteration 66, loss = 0.32924071
Iteration 67, loss = 0.32638299
Iteration 68, loss = 0.32403979
Iteration 69, loss = 0.32334507
Iteration 70, loss = 0.32119341
Iteration 71, loss = 0.32019034
Iteration 72, loss = 0.31800120
Iteration 73, loss = 0.31768728
Iteration 74, loss = 0.31360820
Iteration 75, loss = 0.31202042
Iteration 76, loss = 0.30989591
Iteration 77, loss = 0.30898493
Iteration 78, loss = 0.32200855
Iteration 79, loss = 0.31037675
Iteration 80, loss = 0.30776210
Iteration 81, loss = 0.30617836
Iteration 82, loss = 0.30442339
Iteration 83, loss = 0.30095785
Iteration 84, loss = 0.30094753
Iteration 85, loss = 0.30171418
Iteration 86, loss = 0.29802814
Iteration 87, loss = 0.29797036
Iteration 88, loss = 0.29433687
Iteration 89, loss = 0.29275965
Iteration 90, loss = 0.29165279
Iteration 91, loss = 0.29335578
Iteration 92, loss = 0.29392576
Iteration 93, loss = 0.29301849
Iteration 94, loss = 0.28937965
Iteration 95, loss = 0.28715657
Iteration 96, loss = 0.28455680
Iteration 97, loss = 0.28355884
Iteration 98, loss = 0.29709317
Iteration 99, loss = 0.28414547
Iteration 100, loss = 0.28140870
Iteration 101, loss = 0.28062477
Iteration 102, loss = 0.28032716
Iteration 103, loss = 0.28245794
Iteration 104, loss = 0.29237357
Iteration 105, loss = 0.28014127
Iteration 106, loss = 0.27609446
Iteration 107, loss = 0.27519188
Iteration 108, loss = 0.28061774
Iteration 109, loss = 0.27504060
Iteration 110, loss = 0.27522385
Iteration 111, loss = 0.27253908
Iteration 112, loss = 0.27655693
Iteration 113, loss = 0.27213063
Iteration 114, loss = 0.27037961
Iteration 115, loss = 0.27235967
Iteration 116, loss = 0.27513560
Iteration 117, loss = 0.27008967
Iteration 118, loss = 0.28501459
Iteration 119, loss = 0.27230745
Iteration 120, loss = 0.26737374
Iteration 121, loss = 0.26691356
Iteration 122, loss = 0.26494203
Iteration 123, loss = 0.26422180
Iteration 124, loss = 0.26733697
Iteration 125, loss = 0.26512391
Iteration 126, loss = 0.26740755
Iteration 127, loss = 0.26364286
Iteration 128, loss = 0.26261461
Iteration 129, loss = 0.26095121
Iteration 130, loss = 0.26064684
Iteration 131, loss = 0.26029453
Iteration 132, loss = 0.26055640
Iteration 133, loss = 0.25885636
Iteration 134, loss = 0.25885010
Iteration 135, loss = 0.25899836
Iteration 136, loss = 0.25772571
Iteration 137, loss = 0.25708296
Iteration 138, loss = 0.25965335
Iteration 139, loss = 0.26647165
Iteration 140, loss = 0.25933549
Iteration 141, loss = 0.28890675
Iteration 142, loss = 0.26873165
Iteration 143, loss = 0.26669946
Iteration 144, loss = 0.26687838
Iteration 145, loss = 0.25765244
Iteration 146, loss = 0.25841077
Iteration 147, loss = 0.25445990
Iteration 148, loss = 0.26032826
Iteration 149, loss = 0.25796323
Iteration 150, loss = 0.25283305
Iteration 151, loss = 0.25137191
Iteration 152, loss = 0.26048589
Iteration 153, loss = 0.25715965
Iteration 154, loss = 0.26077513
Iteration 155, loss = 0.25574800
Iteration 156, loss = 0.25075586
Iteration 157, loss = 0.25288515
Iteration 158, loss = 0.25092510
Iteration 159, loss = 0.25272502
Iteration 160, loss = 0.25016759
Iteration 161, loss = 0.25088005
Iteration 162, loss = 0.24939127
Iteration 163, loss = 0.25327328
Iteration 164, loss = 0.25534655
Iteration 165, loss = 0.24993212
Iteration 166, loss = 0.24815752
Iteration 167, loss = 0.25949174
Iteration 168, loss = 0.25018906
Iteration 169, loss = 0.25211692
Iteration 170, loss = 0.26177405
Iteration 171, loss = 0.25080223
Iteration 172, loss = 0.24777854
Iteration 173, loss = 0.24776151
Iteration 174, loss = 0.25183027
Iteration 175, loss = 0.24735472
Iteration 176, loss = 0.24688560
Iteration 177, loss = 0.24511913
Iteration 178, loss = 0.24784192
Iteration 179, loss = 0.24578325
Iteration 180, loss = 0.24378604
Iteration 181, loss = 0.25732799
Iteration 182, loss = 0.24694243
Iteration 183, loss = 0.24459361
Iteration 184, loss = 0.25128112
Iteration 185, loss = 0.24568519
Iteration 186, loss = 0.24936330
Iteration 187, loss = 0.26372002
Iteration 188, loss = 0.24691349
Iteration 189, loss = 0.24358831
Iteration 190, loss = 0.24206958
Iteration 191, loss = 0.24143541
Iteration 192, loss = 0.24208966
Iteration 193, loss = 0.24149836
Iteration 194, loss = 0.24953919
Iteration 195, loss = 0.24809496
Iteration 196, loss = 0.25332857
Iteration 197, loss = 0.24356747
Iteration 198, loss = 0.25081956
Iteration 199, loss = 0.27106870
Iteration 200, loss = 0.25094019
Iteration 201, loss = 0.24598383
Iteration 202, loss = 0.24499507
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 3/5] END ...........alpha=0.1, momentum=0.1;, score=0.905 total time=   7.2s
Iteration 1, loss = 1.14220099
Iteration 2, loss = 1.00866675
Iteration 3, loss = 0.96268035
Iteration 4, loss = 0.93035579
Iteration 5, loss = 0.90392506
Iteration 6, loss = 0.88802597
Iteration 7, loss = 0.86994871
Iteration 8, loss = 0.85345417
Iteration 9, loss = 0.83942340
Iteration 10, loss = 0.83246460
Iteration 11, loss = 0.82591406
Iteration 12, loss = 0.81501290
Iteration 13, loss = 0.82921183
Iteration 14, loss = 0.80073257
Iteration 15, loss = 0.79780963
Iteration 16, loss = 0.79426466
Iteration 17, loss = 0.77879745
Iteration 18, loss = 0.78376233
Iteration 19, loss = 0.77415929
Iteration 20, loss = 0.76910821
Iteration 21, loss = 0.76265001
Iteration 22, loss = 0.76068999
Iteration 23, loss = 0.75935994
Iteration 24, loss = 0.76110740
Iteration 25, loss = 0.75379870
Iteration 26, loss = 0.75826959
Iteration 27, loss = 0.74903350
Iteration 28, loss = 0.74730800
Iteration 29, loss = 0.75806602
Iteration 30, loss = 0.74444911
Iteration 31, loss = 0.74032358
Iteration 32, loss = 0.73435913
Iteration 33, loss = 0.73104683
Iteration 34, loss = 0.73147636
Iteration 35, loss = 0.73216828
Iteration 36, loss = 0.73126650
Iteration 37, loss = 0.72485254
Iteration 38, loss = 0.72649239
Iteration 39, loss = 0.72723040
Iteration 40, loss = 0.72055805
Iteration 41, loss = 0.71815078
Iteration 42, loss = 0.71611103
Iteration 43, loss = 0.71409461
Iteration 44, loss = 0.72603170
Iteration 45, loss = 0.71694257
Iteration 46, loss = 0.72097560
Iteration 47, loss = 0.71374485
Iteration 48, loss = 0.71339320
Iteration 49, loss = 0.71741381
Iteration 50, loss = 0.72265769
Iteration 51, loss = 0.71990889
Iteration 52, loss = 0.71837122
Iteration 53, loss = 0.71226827
Iteration 54, loss = 0.70569722
Iteration 55, loss = 0.70745315
Iteration 56, loss = 0.71321088
Iteration 57, loss = 0.70242538
Iteration 58, loss = 0.70350957
Iteration 59, loss = 0.69923130
Iteration 60, loss = 0.69913918
Iteration 61, loss = 0.70134941
Iteration 62, loss = 0.73013083
Iteration 63, loss = 0.70370494
Iteration 64, loss = 0.71078743
Iteration 65, loss = 0.70584047
Iteration 66, loss = 0.69804971
Iteration 67, loss = 0.69813852
Iteration 68, loss = 0.69448286
Iteration 69, loss = 0.69389110
Iteration 70, loss = 0.70367663
Iteration 71, loss = 0.69221044
Iteration 72, loss = 0.69240251
Iteration 73, loss = 0.69311953
Iteration 74, loss = 0.70466771
Iteration 75, loss = 0.70047115
Iteration 76, loss = 0.69263144
Iteration 77, loss = 0.68826858
Iteration 78, loss = 0.69326978
Iteration 79, loss = 0.68680024
Iteration 80, loss = 0.68433886
Iteration 81, loss = 0.68561323
Iteration 82, loss = 0.68615270
Iteration 83, loss = 0.68412364
Iteration 84, loss = 0.68169383
Iteration 85, loss = 0.68096990
Iteration 86, loss = 0.68140551
Iteration 87, loss = 0.68608141
Iteration 88, loss = 0.68113330
Iteration 89, loss = 0.67851452
Iteration 90, loss = 0.68292401
Iteration 91, loss = 0.68920414
Iteration 92, loss = 0.68064659
Iteration 93, loss = 0.67784430
Iteration 94, loss = 0.68327031
Iteration 95, loss = 0.68748240
Iteration 96, loss = 0.67789196
Iteration 97, loss = 0.67515826
Iteration 98, loss = 0.67791799
Iteration 99, loss = 0.70382768
Iteration 100, loss = 0.68316579
Iteration 101, loss = 0.68237120
Iteration 102, loss = 0.68411419
Iteration 8, loss = 0.70588938
Iteration 9, loss = 0.68142841
Iteration 10, loss = 0.66172266
Iteration 11, loss = 0.65344654
Iteration 12, loss = 0.63222803
Iteration 13, loss = 0.61408531
Iteration 14, loss = 0.59970635
Iteration 15, loss = 0.58527264
Iteration 16, loss = 0.57192408
Iteration 17, loss = 0.56203666
Iteration 18, loss = 0.55259665
Iteration 19, loss = 0.53878856
Iteration 20, loss = 0.54797475
Iteration 21, loss = 0.52889018
Iteration 22, loss = 0.51451285
Iteration 23, loss = 0.50465951
Iteration 24, loss = 0.49606508
Iteration 25, loss = 0.49187157
Iteration 26, loss = 0.48272006
Iteration 27, loss = 0.47535297
Iteration 28, loss = 0.46620997
Iteration 29, loss = 0.46357818
Iteration 30, loss = 0.45386044
Iteration 31, loss = 0.45761254
Iteration 32, loss = 0.44855715
Iteration 33, loss = 0.43840359
Iteration 34, loss = 0.43370828
Iteration 35, loss = 0.42769927
Iteration 36, loss = 0.42169297
Iteration 37, loss = 0.42123000
Iteration 38, loss = 0.41414828
Iteration 39, loss = 0.40972197
Iteration 40, loss = 0.40578946
Iteration 41, loss = 0.39936687
Iteration 42, loss = 0.39484155
Iteration 43, loss = 0.39085919
Iteration 44, loss = 0.38980267
Iteration 45, loss = 0.38604846
Iteration 46, loss = 0.38009053
Iteration 47, loss = 0.37944060
Iteration 48, loss = 0.37245141
Iteration 49, loss = 0.37431219
Iteration 50, loss = 0.36703073
Iteration 51, loss = 0.36204223
Iteration 52, loss = 0.36169628
Iteration 53, loss = 0.35658342
Iteration 54, loss = 0.36091043
Iteration 55, loss = 0.35268947
Iteration 56, loss = 0.34896385
Iteration 57, loss = 0.35488857
Iteration 58, loss = 0.35067315
Iteration 59, loss = 0.34221804
Iteration 60, loss = 0.34427391
Iteration 61, loss = 0.33957186
Iteration 62, loss = 0.33909647
Iteration 63, loss = 0.33632157
Iteration 64, loss = 0.32998894
Iteration 65, loss = 0.32726961
Iteration 66, loss = 0.32528442
Iteration 67, loss = 0.32316540
Iteration 68, loss = 0.32103882
Iteration 69, loss = 0.32287600
Iteration 70, loss = 0.31899012
Iteration 71, loss = 0.31799336
Iteration 72, loss = 0.31612831
Iteration 73, loss = 0.32147562
Iteration 74, loss = 0.31350311
Iteration 75, loss = 0.30985224
Iteration 76, loss = 0.30761803
Iteration 77, loss = 0.30634714
Iteration 78, loss = 0.31650878
Iteration 79, loss = 0.30934147
Iteration 80, loss = 0.30497186
Iteration 81, loss = 0.30483748
Iteration 82, loss = 0.30313879
Iteration 83, loss = 0.29847957
Iteration 84, loss = 0.29828360
Iteration 85, loss = 0.29620908
Iteration 86, loss = 0.29836552
Iteration 87, loss = 0.29789836
Iteration 88, loss = 0.29476566
Iteration 89, loss = 0.29282126
Iteration 90, loss = 0.29074647
Iteration 91, loss = 0.28877902
Iteration 92, loss = 0.29024728
Iteration 93, loss = 0.28992213
Iteration 94, loss = 0.28687361
Iteration 95, loss = 0.28582859
Iteration 96, loss = 0.28368123
Iteration 97, loss = 0.28226488
Iteration 98, loss = 0.29423398
Iteration 99, loss = 0.28219649
Iteration 100, loss = 0.28226518
Iteration 101, loss = 0.28071340
Iteration 102, loss = 0.29432584
Iteration 103, loss = 0.28945216
Iteration 104, loss = 0.28561646
Iteration 105, loss = 0.27702884
Iteration 106, loss = 0.27463160
Iteration 107, loss = 0.27407757
Iteration 108, loss = 0.27935875
Iteration 109, loss = 0.27443086
Iteration 110, loss = 0.27142413
Iteration 111, loss = 0.27552912
Iteration 112, loss = 0.27673705
Iteration 113, loss = 0.27098004
Iteration 114, loss = 0.27205152
Iteration 115, loss = 0.27557907
Iteration 116, loss = 0.27326062
Iteration 117, loss = 0.27557661
Iteration 118, loss = 0.27482676
Iteration 119, loss = 0.27180692
Iteration 120, loss = 0.26932326
Iteration 121, loss = 0.26571092
Iteration 122, loss = 0.26481737
Iteration 123, loss = 0.26293483
Iteration 124, loss = 0.26615797
Iteration 125, loss = 0.26302621
Iteration 126, loss = 0.26436863
Iteration 127, loss = 0.26315815
Iteration 128, loss = 0.26399550
Iteration 129, loss = 0.25985328
Iteration 130, loss = 0.25953544
Iteration 131, loss = 0.25898023
Iteration 132, loss = 0.25989984
Iteration 133, loss = 0.25771944
Iteration 134, loss = 0.25801078
Iteration 135, loss = 0.25951377
Iteration 136, loss = 0.25734349
Iteration 137, loss = 0.25609351
Iteration 138, loss = 0.25757377
Iteration 139, loss = 0.26443073
Iteration 140, loss = 0.25777778
Iteration 141, loss = 0.25837420
Iteration 142, loss = 0.26259779
Iteration 143, loss = 0.26469646
Iteration 144, loss = 0.26056161
Iteration 145, loss = 0.25754817
Iteration 146, loss = 0.25451112
Iteration 147, loss = 0.26078826
Iteration 148, loss = 0.26366796
Iteration 149, loss = 0.25527794
Iteration 150, loss = 0.25143354
Iteration 151, loss = 0.25133198
Iteration 152, loss = 0.25759585
Iteration 153, loss = 0.25339616
Iteration 154, loss = 0.25483189
Iteration 155, loss = 0.25519842
Iteration 156, loss = 0.25107102
Iteration 157, loss = 0.25158808
Iteration 158, loss = 0.25163760
Iteration 159, loss = 0.24948238
Iteration 160, loss = 0.24828631
Iteration 161, loss = 0.24908764
Iteration 162, loss = 0.24795837
Iteration 163, loss = 0.25183192
Iteration 164, loss = 0.25224009
Iteration 165, loss = 0.24833119
Iteration 166, loss = 0.24685180
Iteration 167, loss = 0.26754590
Iteration 168, loss = 0.25323267
Iteration 169, loss = 0.24913752
Iteration 170, loss = 0.25053964
Iteration 171, loss = 0.24976046
Iteration 172, loss = 0.24770078
Iteration 173, loss = 0.24561006
Iteration 174, loss = 0.25009491
Iteration 175, loss = 0.24752813
Iteration 176, loss = 0.24502641
Iteration 177, loss = 0.24442690
Iteration 178, loss = 0.24688863
Iteration 179, loss = 0.24504348
Iteration 180, loss = 0.24354649
Iteration 181, loss = 0.25646442
Iteration 182, loss = 0.24741196
Iteration 183, loss = 0.24399698
Iteration 184, loss = 0.24910693
Iteration 185, loss = 0.24645335
Iteration 186, loss = 0.25287463
Iteration 187, loss = 0.26192633
Iteration 188, loss = 0.24660949
Iteration 189, loss = 0.24578649
Iteration 190, loss = 0.24251305
Iteration 191, loss = 0.24117876
Iteration 192, loss = 0.24195984
Iteration 193, loss = 0.24177150
Iteration 194, loss = 0.24929887
Iteration 195, loss = 0.25039331
Iteration 196, loss = 0.25508373
Iteration 197, loss = 0.24372523
Iteration 198, loss = 0.24117221
Iteration 199, loss = 0.28311231
Iteration 200, loss = 0.25556217
Iteration 201, loss = 0.24491279
Iteration 202, loss = 0.24396982
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 4/5] END ...........alpha=0.1, momentum=0.1;, score=0.906 total time=   6.6s
Iteration 1, loss = 1.14346442
Iteration 2, loss = 1.00572794
Iteration 3, loss = 0.95071380
Iteration 4, loss = 0.91644474
Iteration 5, loss = 0.88842670
Iteration 6, loss = 0.86311429
Iteration 7, loss = 0.84069009
Iteration 8, loss = 0.82226788
Iteration 9, loss = 0.80391066
Iteration 10, loss = 0.79509997
Iteration 11, loss = 0.78689375
Iteration 12, loss = 0.77092291
Iteration 13, loss = 0.75661592
Iteration 14, loss = 0.74976322
Iteration 15, loss = 0.73695917
Iteration 16, loss = 0.72754323
Iteration 17, loss = 0.72054169
Iteration 18, loss = 0.71722031
Iteration 19, loss = 0.70705632
Iteration 20, loss = 0.71538869
Iteration 21, loss = 0.70405054
Iteration 22, loss = 0.69144617
Iteration 23, loss = 0.68538873
Iteration 24, loss = 0.67929245
Iteration 25, loss = 0.67602904
Iteration 26, loss = 0.67052950
Iteration 27, loss = 0.66619987
Iteration 28, loss = 0.66854400
Iteration 29, loss = 0.66117163
Iteration 30, loss = 0.65499802
Iteration 31, loss = 0.65372076
Iteration 32, loss = 0.65095452
Iteration 33, loss = 0.64288057
Iteration 34, loss = 0.64113364
Iteration 35, loss = 0.63912007
Iteration 36, loss = 0.63577539
Iteration 37, loss = 0.63437328
Iteration 38, loss = 0.63098209
Iteration 39, loss = 0.62803534
Iteration 40, loss = 0.62416838
Iteration 41, loss = 0.62048078
Iteration 42, loss = 0.61875134
Iteration 43, loss = 0.61533364
Iteration 44, loss = 0.61484241
Iteration 45, loss = 0.61380088
Iteration 46, loss = 0.60759351
Iteration 133, loss = 0.32934532
Iteration 134, loss = 0.33176183
Iteration 135, loss = 0.33012468
Iteration 136, loss = 0.32943290
Iteration 137, loss = 0.32637814
Iteration 138, loss = 0.32849905
Iteration 139, loss = 0.33111827
Iteration 140, loss = 0.32670297
Iteration 141, loss = 0.32547310
Iteration 142, loss = 0.33029912
Iteration 143, loss = 0.32585920
Iteration 144, loss = 0.34264814
Iteration 145, loss = 0.32835196
Iteration 146, loss = 0.32885226
Iteration 147, loss = 0.32590423
Iteration 148, loss = 0.32321020
Iteration 149, loss = 0.32046188
Iteration 150, loss = 0.32208551
Iteration 151, loss = 0.31910179
Iteration 152, loss = 0.31961447
Iteration 153, loss = 0.31944088
Iteration 154, loss = 0.31740310
Iteration 155, loss = 0.31938611
Iteration 156, loss = 0.31629625
Iteration 157, loss = 0.31557590
Iteration 158, loss = 0.31553997
Iteration 159, loss = 0.31482581
Iteration 160, loss = 0.32037794
Iteration 161, loss = 0.31901274
Iteration 162, loss = 0.32559279
Iteration 163, loss = 0.32229155
Iteration 164, loss = 0.31641029
Iteration 165, loss = 0.31399497
Iteration 166, loss = 0.32938445
Iteration 167, loss = 0.32254921
Iteration 168, loss = 0.31537383
Iteration 169, loss = 0.31276217
Iteration 170, loss = 0.33790353
Iteration 171, loss = 0.32103383
Iteration 172, loss = 0.33826122
Iteration 173, loss = 0.31605284
Iteration 174, loss = 0.32300331
Iteration 175, loss = 0.32278114
Iteration 176, loss = 0.31347886
Iteration 177, loss = 0.32022691
Iteration 178, loss = 0.31884854
Iteration 179, loss = 0.31579409
Iteration 180, loss = 0.31834899
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 1/5] END ...........alpha=0.1, momentum=0.1;, score=0.888 total time=   7.1s
Iteration 1, loss = 1.05746652
Iteration 2, loss = 0.92159099
Iteration 3, loss = 0.86227676
Iteration 4, loss = 0.82345315
Iteration 5, loss = 0.78578906
Iteration 6, loss = 0.75494592
Iteration 7, loss = 0.72601008
Iteration 8, loss = 0.70197675
Iteration 9, loss = 0.67813517
Iteration 10, loss = 0.65569582
Iteration 11, loss = 0.64603641
Iteration 12, loss = 0.62919835
Iteration 13, loss = 0.60772415
Iteration 14, loss = 0.59511366
Iteration 15, loss = 0.57909581
Iteration 16, loss = 0.56527386
Iteration 17, loss = 0.55549771
Iteration 18, loss = 0.54360423
Iteration 19, loss = 0.53142839
Iteration 20, loss = 0.52443457
Iteration 21, loss = 0.51483986
Iteration 22, loss = 0.50425132
Iteration 23, loss = 0.49951297
Iteration 24, loss = 0.48953367
Iteration 25, loss = 0.48350592
Iteration 26, loss = 0.47395194
Iteration 27, loss = 0.46711542
Iteration 28, loss = 0.45841285
Iteration 29, loss = 0.45651765
Iteration 30, loss = 0.44603594
Iteration 31, loss = 0.44401269
Iteration 32, loss = 0.43861459
Iteration 33, loss = 0.42984560
Iteration 34, loss = 0.42328747
Iteration 35, loss = 0.41708539
Iteration 36, loss = 0.41301192
Iteration 37, loss = 0.41152775
Iteration 38, loss = 0.40542806
Iteration 39, loss = 0.40042820
Iteration 40, loss = 0.39475491
Iteration 41, loss = 0.39037019
Iteration 42, loss = 0.38561760
Iteration 43, loss = 0.38048999
Iteration 44, loss = 0.39045523
Iteration 45, loss = 0.37662159
Iteration 46, loss = 0.37018755
Iteration 47, loss = 0.36867731
Iteration 48, loss = 0.36285572
Iteration 49, loss = 0.36444942
Iteration 50, loss = 0.35854980
Iteration 51, loss = 0.35397986
Iteration 52, loss = 0.35106404
Iteration 53, loss = 0.34617088
Iteration 54, loss = 0.35476078
Iteration 55, loss = 0.34459253
Iteration 56, loss = 0.34040523
Iteration 57, loss = 0.34252817
Iteration 58, loss = 0.34122677
Iteration 59, loss = 0.33322107
Iteration 60, loss = 0.33640866
Iteration 61, loss = 0.32875133
Iteration 62, loss = 0.32946941
Iteration 63, loss = 0.32479292
Iteration 64, loss = 0.32040517
Iteration 65, loss = 0.31755596
Iteration 66, loss = 0.31560755
Iteration 67, loss = 0.31364108
Iteration 68, loss = 0.31216996
Iteration 69, loss = 0.31235381
Iteration 70, loss = 0.31069640
Iteration 71, loss = 0.30951930
Iteration 72, loss = 0.30673099
Iteration 73, loss = 0.31192562
Iteration 74, loss = 0.30563523
Iteration 75, loss = 0.30081712
Iteration 76, loss = 0.29830912
Iteration 77, loss = 0.30273657
Iteration 78, loss = 0.31455826
Iteration 79, loss = 0.30510724
Iteration 80, loss = 0.29635319
Iteration 81, loss = 0.29656288
Iteration 82, loss = 0.29599301
Iteration 83, loss = 0.29077277
Iteration 84, loss = 0.28991505
Iteration 85, loss = 0.28867701
Iteration 86, loss = 0.29402473
Iteration 87, loss = 0.29157638
Iteration 88, loss = 0.28557625
Iteration 89, loss = 0.28322001
Iteration 90, loss = 0.28250126
Iteration 91, loss = 0.28548205
Iteration 92, loss = 0.29524070
Iteration 93, loss = 0.28617915
Iteration 94, loss = 0.28074822
Iteration 95, loss = 0.28015957
Iteration 96, loss = 0.27778067
Iteration 97, loss = 0.27493616
Iteration 98, loss = 0.28249422
Iteration 99, loss = 0.27503242
Iteration 100, loss = 0.27635137
Iteration 101, loss = 0.27330290
Iteration 102, loss = 0.28193321
Iteration 103, loss = 0.27822263
Iteration 104, loss = 0.28133844
Iteration 105, loss = 0.27195642
Iteration 106, loss = 0.26878918
Iteration 107, loss = 0.26724921
Iteration 108, loss = 0.27162382
Iteration 109, loss = 0.26701239
Iteration 110, loss = 0.26498565
Iteration 111, loss = 0.26815352
Iteration 112, loss = 0.26512202
Iteration 113, loss = 0.26488714
Iteration 114, loss = 0.26312320
Iteration 115, loss = 0.26494513
Iteration 116, loss = 0.26702315
Iteration 117, loss = 0.26940109
Iteration 118, loss = 0.28229341
Iteration 119, loss = 0.26565650
Iteration 120, loss = 0.26329572
Iteration 121, loss = 0.26074115
Iteration 122, loss = 0.26042960
Iteration 123, loss = 0.25863778
Iteration 124, loss = 0.26879612
Iteration 125, loss = 0.26027355
Iteration 126, loss = 0.26071202
Iteration 127, loss = 0.26309741
Iteration 128, loss = 0.25889131
Iteration 129, loss = 0.25481829
Iteration 130, loss = 0.25413833
Iteration 131, loss = 0.25320557
Iteration 132, loss = 0.25432845
Iteration 133, loss = 0.25216406
Iteration 134, loss = 0.25264780
Iteration 135, loss = 0.25323644
Iteration 136, loss = 0.25209232
Iteration 137, loss = 0.25110940
Iteration 138, loss = 0.25254143
Iteration 139, loss = 0.25831191
Iteration 140, loss = 0.25143771
Iteration 141, loss = 0.25347512
Iteration 142, loss = 0.25803688
Iteration 143, loss = 0.26086167
Iteration 144, loss = 0.25611659
Iteration 145, loss = 0.25235049
Iteration 146, loss = 0.24943665
Iteration 147, loss = 0.25426797
Iteration 148, loss = 0.25435920
Iteration 149, loss = 0.25010611
Iteration 150, loss = 0.24656698
Iteration 151, loss = 0.24591831
Iteration 152, loss = 0.24806633
Iteration 153, loss = 0.25549841
Iteration 154, loss = 0.25040239
Iteration 155, loss = 0.24925035
Iteration 156, loss = 0.24564998
Iteration 157, loss = 0.24542673
Iteration 158, loss = 0.24496244
Iteration 159, loss = 0.24441889
Iteration 160, loss = 0.24367152
Iteration 161, loss = 0.25191423
Iteration 162, loss = 0.24387944
Iteration 163, loss = 0.24822547
Iteration 164, loss = 0.24367756
Iteration 165, loss = 0.24744828
Iteration 166, loss = 0.24632051
Iteration 167, loss = 0.25534967
Iteration 168, loss = 0.24791211
Iteration 169, loss = 0.24282252
Iteration 170, loss = 0.24691511
Iteration 171, loss = 0.24419130
Iteration 172, loss = 0.24203829
Iteration 173, loss = 0.24227072
Iteration 174, loss = 0.24645751
Iteration 175, loss = 0.24199174
Iteration 176, loss = 0.24263651
Iteration 177, loss = 0.23990782
Iteration 178, loss = 0.24139436
Iteration 179, loss = 0.24683082
Iteration 180, loss = 0.24068140
Iteration 181, loss = 0.25066562
Iteration 182, loss = 0.24232045
Iteration 183, loss = 0.24004063
Iteration 184, loss = 0.24063254
Iteration 185, loss = 0.24186341
Iteration 186, loss = 0.23825963
Iteration 187, loss = 0.24344534
Iteration 188, loss = 0.23859361
Iteration 189, loss = 0.24042180
Iteration 190, loss = 0.23776637
Iteration 191, loss = 0.24464724

Iteration 67, loss = 0.65121162
Iteration 68, loss = 0.64653551
Iteration 69, loss = 0.64499065
Iteration 70, loss = 0.66169261
Iteration 71, loss = 0.64504822
Iteration 72, loss = 0.64361544
Iteration 73, loss = 0.64272253
Iteration 74, loss = 0.63823769
Iteration 75, loss = 0.64887617
Iteration 76, loss = 0.64094717
Iteration 77, loss = 0.63624635
Iteration 78, loss = 0.64113830
Iteration 79, loss = 0.63611570
Iteration 80, loss = 0.63252385
Iteration 81, loss = 0.63354500
Iteration 82, loss = 0.63367510
Iteration 83, loss = 0.64880119
Iteration 84, loss = 0.63970127
Iteration 85, loss = 0.63184403
Iteration 86, loss = 0.63070362
Iteration 87, loss = 0.63695082
Iteration 88, loss = 0.62823533
Iteration 89, loss = 0.62457528
Iteration 90, loss = 0.62465413
Iteration 91, loss = 0.62625440
Iteration 92, loss = 0.62468769
Iteration 93, loss = 0.62240851
Iteration 94, loss = 0.61992714
Iteration 95, loss = 0.63165039
Iteration 96, loss = 0.62091653
Iteration 97, loss = 0.61895397
Iteration 98, loss = 0.62266315
Iteration 99, loss = 0.66241231
Iteration 100, loss = 0.62994759
Iteration 101, loss = 0.62980395
Iteration 102, loss = 0.63338210
Iteration 103, loss = 0.62517618
Iteration 104, loss = 0.62399532
Iteration 105, loss = 0.62114626
Iteration 106, loss = 0.61839183
Iteration 107, loss = 0.61677503
Iteration 108, loss = 0.61869607
Iteration 109, loss = 0.61432622
Iteration 110, loss = 0.61494683
Iteration 111, loss = 0.61151918
Iteration 112, loss = 0.61048888
Iteration 113, loss = 0.61098226
Iteration 114, loss = 0.61056723
Iteration 115, loss = 0.61019759
Iteration 116, loss = 0.61245832
Iteration 117, loss = 0.60933659
Iteration 118, loss = 0.61535363
Iteration 119, loss = 0.61735747
Iteration 120, loss = 0.61345409
Iteration 121, loss = 0.61212637
Iteration 122, loss = 0.61677846
Iteration 123, loss = 0.61074903
Iteration 124, loss = 0.60969344
Iteration 125, loss = 0.61622356
Iteration 126, loss = 0.61032617
Iteration 127, loss = 0.60649981
Iteration 128, loss = 0.60995755
Iteration 129, loss = 0.61339872
Iteration 130, loss = 0.60801365
Iteration 131, loss = 0.60796559
Iteration 132, loss = 0.60708697
Iteration 133, loss = 0.60573128
Iteration 134, loss = 0.61457228
Iteration 135, loss = 0.60559287
Iteration 136, loss = 0.60402146
Iteration 137, loss = 0.60370197
Iteration 138, loss = 0.60866335
Iteration 139, loss = 0.61383257
Iteration 140, loss = 0.60847088
Iteration 141, loss = 0.60277096
Iteration 142, loss = 0.60690573
Iteration 143, loss = 0.60736469
Iteration 144, loss = 0.61956255
Iteration 145, loss = 0.60755919
Iteration 146, loss = 0.60796343
Iteration 147, loss = 0.60418407
Iteration 148, loss = 0.60492189
Iteration 149, loss = 0.60053860
Iteration 150, loss = 0.60167595
Iteration 151, loss = 0.59777289
Iteration 152, loss = 0.61164467
Iteration 153, loss = 0.60023053
Iteration 154, loss = 0.59731447
Iteration 155, loss = 0.60388606
Iteration 156, loss = 0.59607263
Iteration 157, loss = 0.59680995
Iteration 158, loss = 0.59709305
Iteration 159, loss = 0.59492896
Iteration 160, loss = 0.59958384
Iteration 161, loss = 0.60017265
Iteration 162, loss = 0.60461143
Iteration 163, loss = 0.59758145
Iteration 164, loss = 0.59464116
Iteration 165, loss = 0.59467946
Iteration 166, loss = 0.59857370
Iteration 167, loss = 0.59592869
Iteration 168, loss = 0.59967842
Iteration 169, loss = 0.59444425
Iteration 170, loss = 0.60667631
Iteration 171, loss = 0.60010284
Iteration 172, loss = 0.60594342
Iteration 173, loss = 0.59522633
Iteration 174, loss = 0.60291336
Iteration 175, loss = 0.59859361
Iteration 176, loss = 0.59523723
Iteration 177, loss = 0.59604351
Iteration 178, loss = 0.59442544
Iteration 179, loss = 0.59600067
Iteration 180, loss = 0.60307257
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 1/5] END ...........alpha=0.4, momentum=0.2;, score=0.780 total time=   5.5s
Iteration 1, loss = 1.05997911
Iteration 2, loss = 0.92445591
Iteration 3, loss = 0.86623134
Iteration 4, loss = 0.82381793
Iteration 5, loss = 0.78737968
Iteration 6, loss = 0.76357433
Iteration 7, loss = 0.73787419
Iteration 8, loss = 0.71445020
Iteration 9, loss = 0.69321060
Iteration 10, loss = 0.68000822
Iteration 11, loss = 0.66589727
Iteration 12, loss = 0.64967163
Iteration 13, loss = 0.65397011
Iteration 14, loss = 0.62540471
Iteration 15, loss = 0.62191946
Iteration 16, loss = 0.61261301
Iteration 17, loss = 0.59464005
Iteration 18, loss = 0.59060667
Iteration 19, loss = 0.58057004
Iteration 20, loss = 0.57175529
Iteration 21, loss = 0.56163108
Iteration 22, loss = 0.55972047
Iteration 23, loss = 0.55693075
Iteration 24, loss = 0.55499142
Iteration 25, loss = 0.54167155
Iteration 26, loss = 0.54078407
Iteration 27, loss = 0.52902906
Iteration 28, loss = 0.52127845
Iteration 29, loss = 0.53669837
Iteration 30, loss = 0.51606481
Iteration 31, loss = 0.50759048
Iteration 32, loss = 0.50179459
Iteration 33, loss = 0.49561157
Iteration 34, loss = 0.49207098
Iteration 35, loss = 0.48966050
Iteration 36, loss = 0.48536480
Iteration 37, loss = 0.47817715
Iteration 38, loss = 0.47665155
Iteration 39, loss = 0.47441740
Iteration 40, loss = 0.46593260
Iteration 41, loss = 0.46152168
Iteration 42, loss = 0.45706002
Iteration 43, loss = 0.45300143
Iteration 44, loss = 0.46366720
Iteration 45, loss = 0.46440659
Iteration 46, loss = 0.44989797
Iteration 47, loss = 0.44850434
Iteration 48, loss = 0.44874887
Iteration 49, loss = 0.45420947
Iteration 50, loss = 0.44289941
Iteration 51, loss = 0.44481652
Iteration 52, loss = 0.44856249
Iteration 53, loss = 0.43492070
Iteration 54, loss = 0.42585249
Iteration 55, loss = 0.42570960
Iteration 56, loss = 0.43213077
Iteration 57, loss = 0.42120962
Iteration 58, loss = 0.42264071
Iteration 59, loss = 0.41359612
Iteration 60, loss = 0.41266963
Iteration 61, loss = 0.40911867
Iteration 62, loss = 0.47020326
Iteration 63, loss = 0.41920091
Iteration 64, loss = 0.40832063
Iteration 65, loss = 0.43604087
Iteration 66, loss = 0.41322087
Iteration 67, loss = 0.40999149
Iteration 68, loss = 0.40199313
Iteration 69, loss = 0.40175190
Iteration 70, loss = 0.40088309
Iteration 71, loss = 0.39348753
Iteration 72, loss = 0.39264613
Iteration 73, loss = 0.39631253
Iteration 74, loss = 0.39902222
Iteration 75, loss = 0.38857406
Iteration 76, loss = 0.38445999
Iteration 77, loss = 0.38274502
Iteration 78, loss = 0.38614090
Iteration 79, loss = 0.38152840
Iteration 80, loss = 0.37809684
Iteration 81, loss = 0.37912363
Iteration 82, loss = 0.37601893
Iteration 83, loss = 0.37780144
Iteration 84, loss = 0.37286732
Iteration 85, loss = 0.37045166
Iteration 86, loss = 0.37505814
Iteration 87, loss = 0.37774955
Iteration 88, loss = 0.37072294
Iteration 89, loss = 0.36596087
Iteration 90, loss = 0.36964865
Iteration 91, loss = 0.38550167
Iteration 92, loss = 0.36685546
Iteration 93, loss = 0.36242367
Iteration 94, loss = 0.36532341
Iteration 95, loss = 0.40345202
Iteration 96, loss = 0.36748613
Iteration 97, loss = 0.35927363
Iteration 98, loss = 0.35889181
Iteration 99, loss = 0.39252949
Iteration 100, loss = 0.36940293
Iteration 101, loss = 0.36322994
Iteration 102, loss = 0.36084064
Iteration 103, loss = 0.35584119
Iteration 104, loss = 0.35183211
Iteration 105, loss = 0.34933338
Iteration 106, loss = 0.34906992
Iteration 107, loss = 0.35453774
Iteration 108, loss = 0.35285058
Iteration 109, loss = 0.34817424
Iteration 110, loss = 0.34416003
Iteration 111, loss = 0.34958149
Iteration 112, loss = 0.34430607
Iteration 113, loss = 0.34900763
Iteration 114, loss = 0.34244847
Iteration 115, loss = 0.34064388
Iteration 116, loss = 0.34339180
Iteration 117, loss = 0.33947494
Iteration 118, loss = 0.34441353
Iteration 119, loss = 0.34803538
Iteration 120, loss = 0.35255799
Iteration 121, loss = 0.34081209
Iteration 122, loss = 0.34883802
Iteration 123, loss = 0.34497930
Iteration 124, loss = 0.33736978
Iteration 125, loss = 0.34970292
Iteration 126, loss = 0.33670733
Iteration 127, loss = 0.33316482
Iteration 128, loss = 0.33575642
Iteration 129, loss = 0.33142443
Iteration 130, loss = 0.33137120
Iteration 131, loss = 0.33005780
Iteration 132, loss = 0.33263527
Iteration 133, loss = 0.32892666
Iteration 134, loss = 0.33030424
Iteration 135, loss = 0.33059225
Iteration 136, loss = 0.32912071
Iteration 137, loss = 0.32660621
Iteration 138, loss = 0.32771314
Iteration 139, loss = 0.32941664
Iteration 140, loss = 0.32631807
Iteration 141, loss = 0.32425537
Iteration 142, loss = 0.32908230
Iteration 143, loss = 0.32700504
Iteration 144, loss = 0.32275260
Iteration 145, loss = 0.32104833
Iteration 146, loss = 0.33727918
Iteration 147, loss = 0.34862239
Iteration 148, loss = 0.32594785
Iteration 149, loss = 0.32115468
Iteration 150, loss = 0.32173919
Iteration 151, loss = 0.31910835
Iteration 152, loss = 0.31887791
Iteration 153, loss = 0.31796736
Iteration 154, loss = 0.31706571
Iteration 155, loss = 0.31790766
Iteration 156, loss = 0.32575093
Iteration 157, loss = 0.32037082
Iteration 158, loss = 0.31814497
Iteration 159, loss = 0.31637134
Iteration 160, loss = 0.31694752
Iteration 161, loss = 0.31955997
Iteration 162, loss = 0.31735968
Iteration 163, loss = 0.31748298
Iteration 164, loss = 0.31455608
Iteration 165, loss = 0.31432381
Iteration 166, loss = 0.32260667
Iteration 167, loss = 0.31742293
Iteration 168, loss = 0.31491220
Iteration 169, loss = 0.31244918
Iteration 170, loss = 0.33110182
Iteration 171, loss = 0.32203852
Iteration 172, loss = 0.33635140
Iteration 173, loss = 0.31628721
Iteration 174, loss = 0.31890294
Iteration 175, loss = 0.31230766
Iteration 176, loss = 0.30865692
Iteration 177, loss = 0.31756139
Iteration 178, loss = 0.31789753
Iteration 179, loss = 0.31077431
Iteration 180, loss = 0.31922442
Iteration 181, loss = 0.31091787
Iteration 182, loss = 0.31541549
Iteration 183, loss = 0.31891182
Iteration 184, loss = 0.30970376
Iteration 185, loss = 0.30771090
Iteration 186, loss = 0.30637577
Iteration 187, loss = 0.31409898
Iteration 188, loss = 0.30712518
Iteration 189, loss = 0.30518312
Iteration 190, loss = 0.31616659
Iteration 191, loss = 0.30627441
Iteration 192, loss = 0.31403145
Iteration 193, loss = 0.32300493
Iteration 194, loss = 0.31737009
Iteration 195, loss = 0.30853317
Iteration 196, loss = 0.30804296
Iteration 197, loss = 0.30447554
Iteration 198, loss = 0.30567118
Iteration 199, loss = 0.30324948
Iteration 200, loss = 0.33174322
Iteration 201, loss = 0.31827027
Iteration 202, loss = 0.30957212
Iteration 203, loss = 0.30590623
Iteration 204, loss = 0.30260191
Iteration 205, loss = 0.30173826
Iteration 206, loss = 0.30223733
Iteration 207, loss = 0.30626331
Iteration 208, loss = 0.30387384
Iteration 209, loss = 0.30083023
Iteration 210, loss = 0.30094134
Iteration 211, loss = 0.30116024
Iteration 212, loss = 0.29911788
Iteration 213, loss = 0.31073134
Iteration 214, loss = 0.30217084
Iteration 215, loss = 0.29948774
Iteration 216, loss = 0.29970427
Iteration 217, loss = 0.29826298
Iteration 218, loss = 0.31383435
Iteration 219, loss = 0.30092210
Iteration 220, loss = 0.29960959
Iteration 221, loss = 0.29793279
Iteration 222, loss = 0.31369697
Iteration 223, loss = 0.30146522
Iteration 224, loss = 0.29780945
Iteration 225, loss = 0.29766314
Iteration 226, loss = 0.29683051
Iteration 227, loss = 0.30332995
Iteration 228, loss = 0.32136061
Iteration 229, loss = 0.30173526
Iteration 230, loss = 0.29824698
Iteration 231, loss = 0.29698973
Iteration 232, loss = 0.29649631
Iteration 233, loss = 0.29971810
Iteration 234, loss = 0.30652602
Iteration 235, loss = 0.30373008
Iteration 236, loss = 0.29837399
Iteration 237, loss = 0.29651404
Iteration 238, loss = 0.29672172
Iteration 239, loss = 0.30063545
Iteration 240, loss = 0.29671523
Iteration 241, loss = 0.30060251
Iteration 242, loss = 0.30143177
Iteration 243, loss = 0.30093920
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 2/5] END ...........alpha=0.1, momentum=0.1;, score=0.896 total time=   9.3s
Iteration 1, loss = 1.14774612
Iteration 2, loss = 1.01362964
Iteration 3, loss = 0.96661438
Iteration 4, loss = 0.93409756
Iteration 5, loss = 0.91891423
Iteration 6, loss = 0.89597580
Iteration 7, loss = 0.87621202
Iteration 8, loss = 0.86135769
Iteration 9, loss = 0.84770709
Iteration 10, loss = 0.83962690
Iteration 11, loss = 0.83494028
Iteration 12, loss = 0.82431765
Iteration 13, loss = 0.83958540
Iteration 14, loss = 0.80877772
Iteration 15, loss = 0.79873968
Iteration 16, loss = 0.79961068
Iteration 17, loss = 0.78552842
Iteration 18, loss = 0.79245637
Iteration 19, loss = 0.78077811
Iteration 20, loss = 0.77619758
Iteration 21, loss = 0.76946026
Iteration 22, loss = 0.76440999
Iteration 23, loss = 0.76612179
Iteration 24, loss = 0.76524802
Iteration 25, loss = 0.75939283
Iteration 26, loss = 0.76210292
Iteration 27, loss = 0.75474148
Iteration 28, loss = 0.75037600
Iteration 29, loss = 0.76006771
Iteration 30, loss = 0.74814286
Iteration 31, loss = 0.74392467
Iteration 32, loss = 0.73829901
Iteration 33, loss = 0.73526810
Iteration 34, loss = 0.73596381
Iteration 35, loss = 0.73663604
Iteration 36, loss = 0.73142871
Iteration 37, loss = 0.72699764
Iteration 38, loss = 0.72444914
Iteration 39, loss = 0.72526396
Iteration 40, loss = 0.72067303
Iteration 41, loss = 0.72217039
Iteration 42, loss = 0.72022283
Iteration 43, loss = 0.71729894
Iteration 44, loss = 0.72775677
Iteration 45, loss = 0.72103396
Iteration 46, loss = 0.72618714
Iteration 47, loss = 0.71727843
Iteration 48, loss = 0.71441321
Iteration 49, loss = 0.71986760
Iteration 50, loss = 0.72503580
Iteration 51, loss = 0.71781212
Iteration 52, loss = 0.72306128
Iteration 53, loss = 0.71442963
Iteration 54, loss = 0.70838708
Iteration 55, loss = 0.70429537
Iteration 56, loss = 0.71559630
Iteration 57, loss = 0.70354148
Iteration 58, loss = 0.70593751
Iteration 59, loss = 0.70078258
Iteration 60, loss = 0.69999536
Iteration 61, loss = 0.70298075
Iteration 62, loss = 0.72506661
Iteration 63, loss = 0.70353749
Iteration 64, loss = 0.69805742
Iteration 65, loss = 0.71127065
Iteration 66, loss = 0.70061796
Iteration 67, loss = 0.69959394
Iteration 68, loss = 0.69587761
Iteration 69, loss = 0.69472406
Iteration 70, loss = 0.70833090
Iteration 71, loss = 0.69513157
Iteration 72, loss = 0.69424201
Iteration 73, loss = 0.69349543
Iteration 74, loss = 0.68974642
Iteration 75, loss = 0.69961469
Iteration 76, loss = 0.69270370
Iteration 77, loss = 0.68827559
Iteration 78, loss = 0.69437587
Iteration 79, loss = 0.68842352
Iteration 80, loss = 0.68545375
Iteration 81, loss = 0.68587937
Iteration 82, loss = 0.68688272
Iteration 83, loss = 0.69825838
Iteration 84, loss = 0.69085883
Iteration 85, loss = 0.68461432
Iteration 86, loss = 0.68361656
Iteration 87, loss = 0.69059629
Iteration 88, loss = 0.68221730
Iteration 89, loss = 0.67925003
Iteration 90, loss = 0.67911238
Iteration 91, loss = 0.68093811
Iteration 92, loss = 0.68022617
Iteration 93, loss = 0.67807450
Iteration 94, loss = 0.67557698
Iteration 95, loss = 0.68536624
Iteration 96, loss = 0.67666422
Iteration 97, loss = 0.67508259
Iteration 98, loss = 0.67811015
Iteration 99, loss = 0.71442649
Iteration 100, loss = 0.68439196
Iteration 101, loss = 0.68417487
Iteration 102, loss = 0.68651957
Iteration 103, loss = 0.68051322
Iteration 104, loss = 0.68066194
Iteration 105, loss = 0.67787636
Iteration 106, loss = 0.67592335
Iteration 107, loss = 0.67336453
Iteration 108, loss = 0.67505972
Iteration 109, loss = 0.67108153
Iteration 110, loss = 0.67332972
Iteration 111, loss = 0.66843364
Iteration 112, loss = 0.66824460
Iteration 113, loss = 0.66771559
Iteration 114, loss = 0.66849622
Iteration 115, loss = 0.66773307
Iteration 116, loss = 0.66930052
Iteration 117, loss = 0.66683962
Iteration 118, loss = 0.67352612
Iteration 119, loss = 0.67491858
Iteration 120, loss = 0.67023032
Iteration 121, loss = 0.67013413
Iteration 122, loss = 0.67399736

Iteration 103, loss = 0.67980322
Iteration 104, loss = 0.68168086
Iteration 105, loss = 0.67833906
Iteration 106, loss = 0.68095689
Iteration 107, loss = 0.67782176
Iteration 108, loss = 0.68207695
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 2/5] END ...........alpha=0.5, momentum=0.4;, score=0.741 total time=   3.2s
Iteration 1, loss = 1.14325847
Iteration 2, loss = 1.00757690
Iteration 3, loss = 0.95335641
Iteration 4, loss = 0.91823161
Iteration 5, loss = 0.88932930
Iteration 6, loss = 0.86393264
Iteration 7, loss = 0.83983498
Iteration 8, loss = 0.82216284
Iteration 9, loss = 0.80389182
Iteration 10, loss = 0.79005567
Iteration 11, loss = 0.78645546
Iteration 12, loss = 0.76891571
Iteration 13, loss = 0.75591639
Iteration 14, loss = 0.74688545
Iteration 15, loss = 0.73585687
Iteration 16, loss = 0.72667247
Iteration 17, loss = 0.72028535
Iteration 18, loss = 0.71681034
Iteration 19, loss = 0.70495015
Iteration 20, loss = 0.71270319
Iteration 21, loss = 0.70247108
Iteration 22, loss = 0.69089877
Iteration 23, loss = 0.68361448
Iteration 24, loss = 0.67780304
Iteration 25, loss = 0.67518194
Iteration 26, loss = 0.66958728
Iteration 27, loss = 0.66607612
Iteration 28, loss = 0.65979415
Iteration 29, loss = 0.65772695
Iteration 30, loss = 0.65133683
Iteration 31, loss = 0.65581265
Iteration 32, loss = 0.65067806
Iteration 33, loss = 0.64214252
Iteration 34, loss = 0.64039287
Iteration 35, loss = 0.63607166
Iteration 36, loss = 0.63429539
Iteration 37, loss = 0.63225420
Iteration 38, loss = 0.62887886
Iteration 39, loss = 0.62486308
Iteration 40, loss = 0.62383550
Iteration 41, loss = 0.61849168
Iteration 42, loss = 0.61664049
Iteration 43, loss = 0.61483081
Iteration 44, loss = 0.61405985
Iteration 45, loss = 0.61047809
Iteration 46, loss = 0.60641024
Iteration 47, loss = 0.60722219
Iteration 48, loss = 0.60114519
Iteration 49, loss = 0.60213784
Iteration 50, loss = 0.59748962
Iteration 51, loss = 0.59568424
Iteration 52, loss = 0.59566263
Iteration 53, loss = 0.58978986
Iteration 54, loss = 0.59665135
Iteration 55, loss = 0.59015539
Iteration 56, loss = 0.58903645
Iteration 57, loss = 0.59031319
Iteration 58, loss = 0.58608669
Iteration 59, loss = 0.58108367
Iteration 60, loss = 0.58561589
Iteration 61, loss = 0.58080350
Iteration 62, loss = 0.58062384
Iteration 63, loss = 0.58558896
Iteration 64, loss = 0.57540393
Iteration 65, loss = 0.57300732
Iteration 66, loss = 0.57208234
Iteration 67, loss = 0.57124565
Iteration 68, loss = 0.56833755
Iteration 69, loss = 0.57387979
Iteration 70, loss = 0.57005503
Iteration 71, loss = 0.56879806
Iteration 72, loss = 0.56670275
Iteration 73, loss = 0.57287579
Iteration 74, loss = 0.56549034
Iteration 75, loss = 0.56237805
Iteration 76, loss = 0.56002886
Iteration 77, loss = 0.56059525
Iteration 78, loss = 0.56512206
Iteration 79, loss = 0.57209937
Iteration 80, loss = 0.56185036
Iteration 81, loss = 0.56172903
Iteration 82, loss = 0.56237377
Iteration 83, loss = 0.55725702
Iteration 84, loss = 0.55873162
Iteration 85, loss = 0.55432665
Iteration 86, loss = 0.55751998
Iteration 87, loss = 0.55590291
Iteration 88, loss = 0.55467282
Iteration 89, loss = 0.55557880
Iteration 90, loss = 0.56017063
Iteration 91, loss = 0.55551142
Iteration 92, loss = 0.55132720
Iteration 93, loss = 0.55118952
Iteration 94, loss = 0.54899504
Iteration 95, loss = 0.55139149
Iteration 96, loss = 0.54816493
Iteration 97, loss = 0.54708208
Iteration 98, loss = 0.55094180
Iteration 99, loss = 0.54777306
Iteration 100, loss = 0.54792211
Iteration 101, loss = 0.55145529
Iteration 102, loss = 0.56184936
Iteration 103, loss = 0.55463176
Iteration 104, loss = 0.55064343
Iteration 105, loss = 0.54453761
Iteration 106, loss = 0.54359227
Iteration 107, loss = 0.54309828
Iteration 108, loss = 0.54669175
Iteration 109, loss = 0.54229776
Iteration 110, loss = 0.53946229
Iteration 111, loss = 0.54278738
Iteration 112, loss = 0.54572040
Iteration 113, loss = 0.54168500
Iteration 114, loss = 0.54036094
Iteration 115, loss = 0.54498631
Iteration 116, loss = 0.54394694
Iteration 117, loss = 0.54774881
Iteration 118, loss = 0.55059885
Iteration 119, loss = 0.54355994
Iteration 120, loss = 0.54230336
Iteration 121, loss = 0.54147349
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 4/5] END ...........alpha=0.5, momentum=0.4;, score=0.826 total time=   3.6s
Iteration 1, loss = 1.10310650
Iteration 2, loss = 0.96787119
Iteration 3, loss = 0.91120025
Iteration 4, loss = 0.87548943
Iteration 5, loss = 0.84549899
Iteration 6, loss = 0.81891527
Iteration 7, loss = 0.79499223
Iteration 8, loss = 0.77522975
Iteration 9, loss = 0.75536528
Iteration 10, loss = 0.74473820
Iteration 11, loss = 0.73620104
Iteration 12, loss = 0.71866206
Iteration 13, loss = 0.70268776
Iteration 14, loss = 0.69389268
Iteration 15, loss = 0.68016561
Iteration 16, loss = 0.66923691
Iteration 17, loss = 0.66079573
Iteration 18, loss = 0.65597530
Iteration 19, loss = 0.64437952
Iteration 20, loss = 0.65370921
Iteration 21, loss = 0.63941100
Iteration 22, loss = 0.62501891
Iteration 23, loss = 0.61777785
Iteration 24, loss = 0.61045423
Iteration 25, loss = 0.60641061
Iteration 26, loss = 0.59954078
Iteration 27, loss = 0.59378320
Iteration 28, loss = 0.59582281
Iteration 29, loss = 0.58704562
Iteration 30, loss = 0.57948527
Iteration 31, loss = 0.57750772
Iteration 32, loss = 0.57368026
Iteration 33, loss = 0.56448724
Iteration 34, loss = 0.56224537
Iteration 35, loss = 0.55929356
Iteration 36, loss = 0.55481872
Iteration 37, loss = 0.55420298
Iteration 38, loss = 0.54873316
Iteration 39, loss = 0.54553191
Iteration 40, loss = 0.54016836
Iteration 41, loss = 0.53565292
Iteration 42, loss = 0.53237415
Iteration 43, loss = 0.52869689
Iteration 44, loss = 0.52858888
Iteration 45, loss = 0.52722133
Iteration 46, loss = 0.51995958
Iteration 47, loss = 0.52047851
Iteration 48, loss = 0.51383470
Iteration 49, loss = 0.51150289
Iteration 50, loss = 0.50885876
Iteration 51, loss = 0.50449608
Iteration 52, loss = 0.50179656
Iteration 53, loss = 0.50028233
Iteration 54, loss = 0.50266584
Iteration 55, loss = 0.49580777
Iteration 56, loss = 0.49291902
Iteration 57, loss = 0.49812379
Iteration 58, loss = 0.49447378
Iteration 59, loss = 0.48854233
Iteration 60, loss = 0.49176305
Iteration 61, loss = 0.48701837
Iteration 62, loss = 0.48073651
Iteration 63, loss = 0.49126403
Iteration 64, loss = 0.47926247
Iteration 65, loss = 0.47579476
Iteration 66, loss = 0.47703763
Iteration 67, loss = 0.47226942
Iteration 68, loss = 0.47030791
Iteration 69, loss = 0.47176200
Iteration 70, loss = 0.47051251
Iteration 71, loss = 0.46726914
Iteration 72, loss = 0.46609883
Iteration 73, loss = 0.46484496
Iteration 74, loss = 0.46238349
Iteration 75, loss = 0.46115339
Iteration 76, loss = 0.45960957
Iteration 77, loss = 0.45914442
Iteration 78, loss = 0.47046518
Iteration 79, loss = 0.46069275
Iteration 80, loss = 0.45886181
Iteration 81, loss = 0.45715029
Iteration 82, loss = 0.45791867
Iteration 83, loss = 0.45322120
Iteration 84, loss = 0.45574512
Iteration 85, loss = 0.45546501
Iteration 86, loss = 0.45119091
Iteration 87, loss = 0.44972878
Iteration 88, loss = 0.45124649
Iteration 89, loss = 0.45045053
Iteration 90, loss = 0.45204544
Iteration 91, loss = 0.44713319
Iteration 92, loss = 0.44715189
Iteration 93, loss = 0.44649700
Iteration 94, loss = 0.44391282
Iteration 95, loss = 0.44345620
Iteration 96, loss = 0.44062699
Iteration 97, loss = 0.43931276
Iteration 98, loss = 0.44958186
Iteration 99, loss = 0.44143235
Iteration 100, loss = 0.43921447
Iteration 101, loss = 0.44099689
Iteration 102, loss = 0.44005439
Iteration 103, loss = 0.44300970
Iteration 104, loss = 0.44657200
Iteration 105, loss = 0.43924343
Iteration 106, loss = 0.43361920
Iteration 107, loss = 0.43282722
Iteration 108, loss = 0.44469607
Iteration 109, loss = 0.43376970
Iteration 110, loss = 0.43216587
Iteration 111, loss = 0.43022118Iteration 192, loss = 0.23910609
Iteration 193, loss = 0.25191606
Iteration 194, loss = 0.24640490
Iteration 195, loss = 0.25040771
Iteration 196, loss = 0.24396950
Iteration 197, loss = 0.23836493
Iteration 198, loss = 0.23623041
Iteration 199, loss = 0.24748864
Iteration 200, loss = 0.24062909
Iteration 201, loss = 0.24085893
Iteration 202, loss = 0.23865508
Iteration 203, loss = 0.24252642
Iteration 204, loss = 0.23704408
Iteration 205, loss = 0.23899547
Iteration 206, loss = 0.23781570
Iteration 207, loss = 0.25460451
Iteration 208, loss = 0.25946341
Iteration 209, loss = 0.23855013
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 5/5] END ...........alpha=0.1, momentum=0.1;, score=0.909 total time=   6.6s
Iteration 1, loss = 1.13960580
Iteration 2, loss = 1.00221420
Iteration 3, loss = 0.94920459
Iteration 4, loss = 0.91638729
Iteration 5, loss = 0.88566899
Iteration 6, loss = 0.86138802
Iteration 7, loss = 0.83881453
Iteration 8, loss = 0.82124532
Iteration 9, loss = 0.80407644
Iteration 10, loss = 0.78783209
Iteration 11, loss = 0.78331788
Iteration 12, loss = 0.76984794
Iteration 13, loss = 0.75464530
Iteration 14, loss = 0.74716947
Iteration 15, loss = 0.73507079
Iteration 16, loss = 0.72512408
Iteration 17, loss = 0.71907336
Iteration 18, loss = 0.71198804
Iteration 19, loss = 0.70244981
Iteration 20, loss = 0.69850746
Iteration 21, loss = 0.69226622
Iteration 22, loss = 0.68557731
Iteration 23, loss = 0.68301778
Iteration 24, loss = 0.67598883
Iteration 25, loss = 0.67241630
Iteration 26, loss = 0.66564080
Iteration 27, loss = 0.66229378
Iteration 28, loss = 0.65656526
Iteration 29, loss = 0.65426738
Iteration 30, loss = 0.64853317
Iteration 31, loss = 0.64902417
Iteration 32, loss = 0.64532509
Iteration 33, loss = 0.63852304
Iteration 34, loss = 0.63452587
Iteration 35, loss = 0.62940738
Iteration 36, loss = 0.62970632
Iteration 37, loss = 0.62589842
Iteration 38, loss = 0.62396855
Iteration 39, loss = 0.61920129
Iteration 40, loss = 0.61838435
Iteration 41, loss = 0.61394582
Iteration 42, loss = 0.61233868
Iteration 43, loss = 0.60789631
Iteration 44, loss = 0.61558340
Iteration 45, loss = 0.60644376
Iteration 46, loss = 0.60126058
Iteration 47, loss = 0.60091762
Iteration 48, loss = 0.59692060
Iteration 49, loss = 0.59686588
Iteration 50, loss = 0.59508377
Iteration 51, loss = 0.59129679
Iteration 52, loss = 0.59267619
Iteration 53, loss = 0.58581376
Iteration 54, loss = 0.59378815
Iteration 55, loss = 0.58724593
Iteration 56, loss = 0.58457526
Iteration 57, loss = 0.58465605
Iteration 58, loss = 0.58248243
Iteration 59, loss = 0.57636000
Iteration 60, loss = 0.58294308
Iteration 61, loss = 0.57555392
Iteration 62, loss = 0.57535827
Iteration 63, loss = 0.57649868
Iteration 64, loss = 0.57049407
Iteration 65, loss = 0.56785571
Iteration 66, loss = 0.56637542
Iteration 67, loss = 0.56491273
Iteration 68, loss = 0.56422317
Iteration 69, loss = 0.57024079
Iteration 70, loss = 0.56615957
Iteration 71, loss = 0.56375362
Iteration 72, loss = 0.56199803
Iteration 73, loss = 0.57009686
Iteration 74, loss = 0.56410919
Iteration 75, loss = 0.55876334
Iteration 76, loss = 0.55558826
Iteration 77, loss = 0.55689142
Iteration 78, loss = 0.56557626
Iteration 79, loss = 0.56848699
Iteration 80, loss = 0.55513985
Iteration 81, loss = 0.55645644
Iteration 82, loss = 0.55768946
Iteration 83, loss = 0.55288614
Iteration 84, loss = 0.55326877
Iteration 85, loss = 0.55106717
Iteration 86, loss = 0.55477783
Iteration 87, loss = 0.55636814
Iteration 88, loss = 0.54972489
Iteration 89, loss = 0.54673689
Iteration 90, loss = 0.55732415
Iteration 91, loss = 0.55461667
Iteration 92, loss = 0.55720901
Iteration 93, loss = 0.54912081
Iteration 94, loss = 0.54603531
Iteration 95, loss = 0.55068308
Iteration 96, loss = 0.54886925
Iteration 97, loss = 0.54431418
Iteration 98, loss = 0.54641347
Iteration 99, loss = 0.54471262
Iteration 100, loss = 0.54448756
Iteration 101, loss = 0.54861444
Iteration 102, loss = 0.55875119
Iteration 103, loss = 0.55181620
Iteration 104, loss = 0.54697407
Iteration 105, loss = 0.54276434
Iteration 106, loss = 0.54012676
Iteration 107, loss = 0.53867075
Iteration 108, loss = 0.54044035
Iteration 109, loss = 0.53863847
Iteration 110, loss = 0.53595471
Iteration 111, loss = 0.53732976
Iteration 112, loss = 0.53921757
Iteration 113, loss = 0.53872257
Iteration 114, loss = 0.53535502
Iteration 115, loss = 0.53918634
Iteration 116, loss = 0.53737604
Iteration 117, loss = 0.54317741
Iteration 118, loss = 0.55378104
Iteration 119, loss = 0.53979419
Iteration 120, loss = 0.53935841
Iteration 121, loss = 0.53831711
Iteration 122, loss = 0.53718759
Iteration 123, loss = 0.53857811
Iteration 124, loss = 0.53770568
Iteration 125, loss = 0.53486118
Iteration 126, loss = 0.53399848
Iteration 127, loss = 0.53529006
Iteration 128, loss = 0.53830682
Iteration 129, loss = 0.53168062
Iteration 130, loss = 0.53119611
Iteration 131, loss = 0.53366896
Iteration 132, loss = 0.53525927
Iteration 133, loss = 0.53059861
Iteration 134, loss = 0.52990423
Iteration 135, loss = 0.53513550
Iteration 136, loss = 0.52985990
Iteration 137, loss = 0.53061359
Iteration 138, loss = 0.53523605
Iteration 139, loss = 0.53151353
Iteration 140, loss = 0.52897808
Iteration 141, loss = 0.53197279
Iteration 142, loss = 0.53207729
Iteration 143, loss = 0.53237668
Iteration 144, loss = 0.53705337
Iteration 145, loss = 0.53206730
Iteration 146, loss = 0.53583015
Iteration 147, loss = 0.53924356
Iteration 148, loss = 0.54951149
Iteration 149, loss = 0.53579029
Iteration 150, loss = 0.52988582
Iteration 151, loss = 0.52742381
Iteration 152, loss = 0.53167670
Iteration 153, loss = 0.53088809
Iteration 154, loss = 0.52833301
Iteration 155, loss = 0.52767123
Iteration 156, loss = 0.52621594
Iteration 157, loss = 0.52920969
Iteration 158, loss = 0.52747789
Iteration 159, loss = 0.52865820
Iteration 160, loss = 0.52564541
Iteration 161, loss = 0.53729101
Iteration 162, loss = 0.52971912
Iteration 163, loss = 0.52651621
Iteration 164, loss = 0.52503127
Iteration 165, loss = 0.52518155
Iteration 166, loss = 0.52651646
Iteration 167, loss = 0.53647755
Iteration 168, loss = 0.53073138
Iteration 169, loss = 0.52591332
Iteration 170, loss = 0.52669863
Iteration 171, loss = 0.52956486
Iteration 172, loss = 0.52657398
Iteration 173, loss = 0.52844795
Iteration 174, loss = 0.52739992
Iteration 175, loss = 0.52495328
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 5/5] END ...........alpha=0.5, momentum=0.4;, score=0.830 total time=   5.2s
Iteration 1, loss = 1.10290825
Iteration 2, loss = 0.96984678
Iteration 3, loss = 0.91397349
Iteration 4, loss = 0.87725607
Iteration 5, loss = 0.84662995
Iteration 6, loss = 0.81945589
Iteration 7, loss = 0.79375996
Iteration 8, loss = 0.77456077
Iteration 9, loss = 0.75433764
Iteration 10, loss = 0.73880764
Iteration 11, loss = 0.73402137
Iteration 12, loss = 0.71511795
Iteration 13, loss = 0.70051271
Iteration 14, loss = 0.68955302
Iteration 15, loss = 0.67722328
Iteration 16, loss = 0.66653335
Iteration 17, loss = 0.65921398
Iteration 18, loss = 0.65383297
Iteration 19, loss = 0.64069431
Iteration 20, loss = 0.64980766
Iteration 21, loss = 0.63571835
Iteration 22, loss = 0.62275085
Iteration 23, loss = 0.61429995
Iteration 24, loss = 0.60746916
Iteration 25, loss = 0.60408555
Iteration 26, loss = 0.59717324
Iteration 27, loss = 0.59202653
Iteration 28, loss = 0.58451402
Iteration 29, loss = 0.58277969
Iteration 30, loss = 0.57457824
Iteration 31, loss = 0.57921164
Iteration 32, loss = 0.57183343
Iteration 33, loss = 0.56246723
Iteration 34, loss = 0.55991773
Iteration 35, loss = 0.55471793
Iteration 36, loss = 0.55104799
Iteration 37, loss = 0.55112540
Iteration 38, loss = 0.54524959
Iteration 39, loss = 0.54051650
Iteration 40, loss = 0.53798412
Iteration 41, loss = 0.53215640
Iteration 42, loss = 0.52919321
Iteration 43, loss = 0.52636272
Iteration 47, loss = 0.60872561
Iteration 48, loss = 0.60301286
Iteration 49, loss = 0.60147930
Iteration 50, loss = 0.60024897
Iteration 51, loss = 0.59673627
Iteration 52, loss = 0.59299297
Iteration 53, loss = 0.59131925
Iteration 54, loss = 0.59411320
Iteration 55, loss = 0.58796643
Iteration 56, loss = 0.58577625
Iteration 57, loss = 0.59004872
Iteration 58, loss = 0.58740949
Iteration 59, loss = 0.58300273
Iteration 60, loss = 0.58698436
Iteration 61, loss = 0.58271227
Iteration 62, loss = 0.57666150
Iteration 63, loss = 0.58777821
Iteration 64, loss = 0.57663344
Iteration 65, loss = 0.57323703
Iteration 66, loss = 0.57559599
Iteration 67, loss = 0.57054592
Iteration 68, loss = 0.56886327
Iteration 69, loss = 0.57105147
Iteration 70, loss = 0.57031321
Iteration 71, loss = 0.56691782
Iteration 72, loss = 0.56616469
Iteration 73, loss = 0.56470454
Iteration 74, loss = 0.56388596
Iteration 75, loss = 0.56313462
Iteration 76, loss = 0.56137024
Iteration 77, loss = 0.56132984
Iteration 78, loss = 0.56944603
Iteration 79, loss = 0.56218727
Iteration 80, loss = 0.56176740
Iteration 81, loss = 0.55938306
Iteration 82, loss = 0.56000106
Iteration 83, loss = 0.55701110
Iteration 84, loss = 0.55952047
Iteration 85, loss = 0.55949443
Iteration 86, loss = 0.55569502
Iteration 87, loss = 0.55370753
Iteration 88, loss = 0.55658586
Iteration 89, loss = 0.55530822
Iteration 90, loss = 0.56278016
Iteration 91, loss = 0.55348309
Iteration 92, loss = 0.55218315
Iteration 93, loss = 0.55158654
Iteration 94, loss = 0.54997632
Iteration 95, loss = 0.55036645
Iteration 96, loss = 0.54726928
Iteration 97, loss = 0.54643645
Iteration 98, loss = 0.55349981
Iteration 99, loss = 0.54958825
Iteration 100, loss = 0.54952272
Iteration 101, loss = 0.55176947
Iteration 102, loss = 0.55001159
Iteration 103, loss = 0.54991782
Iteration 104, loss = 0.55268400
Iteration 105, loss = 0.54749033
Iteration 106, loss = 0.54209740
Iteration 107, loss = 0.54145256
Iteration 108, loss = 0.55143538
Iteration 109, loss = 0.54185739
Iteration 110, loss = 0.54159608
Iteration 111, loss = 0.53965488
Iteration 112, loss = 0.54428787
Iteration 113, loss = 0.54142012
Iteration 114, loss = 0.53937051
Iteration 115, loss = 0.54198061
Iteration 116, loss = 0.54241210
Iteration 117, loss = 0.53916160
Iteration 118, loss = 0.54528120
Iteration 119, loss = 0.54095698
Iteration 120, loss = 0.54132515
Iteration 121, loss = 0.54178329
Iteration 122, loss = 0.53823412
Iteration 123, loss = 0.53775547
Iteration 124, loss = 0.53761053
Iteration 125, loss = 0.53607710
Iteration 126, loss = 0.53856968
Iteration 127, loss = 0.53652722
Iteration 128, loss = 0.53788136
Iteration 129, loss = 0.53350184
Iteration 130, loss = 0.53322045
Iteration 131, loss = 0.53793731
Iteration 132, loss = 0.53848308
Iteration 133, loss = 0.53363648
Iteration 134, loss = 0.53504487
Iteration 135, loss = 0.53338372
Iteration 136, loss = 0.53198264
Iteration 137, loss = 0.53133617
Iteration 138, loss = 0.53532037
Iteration 139, loss = 0.53475338
Iteration 140, loss = 0.53241936
Iteration 141, loss = 0.55809735
Iteration 142, loss = 0.53838631
Iteration 143, loss = 0.53443520
Iteration 144, loss = 0.54436197
Iteration 145, loss = 0.53472916
Iteration 146, loss = 0.54429053
Iteration 147, loss = 0.53294300
Iteration 148, loss = 0.54796563
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 3/5] END ...........alpha=0.5, momentum=0.4;, score=0.812 total time=   4.4s
Iteration 1, loss = 1.10181513
Iteration 2, loss = 0.96950996
Iteration 3, loss = 0.92006965
Iteration 4, loss = 0.88526150
Iteration 5, loss = 0.85626470
Iteration 6, loss = 0.83860195
Iteration 7, loss = 0.81875463
Iteration 8, loss = 0.80057328
Iteration 9, loss = 0.78464320
Iteration 10, loss = 0.77621170
Iteration 11, loss = 0.76785168
Iteration 12, loss = 0.75543097
Iteration 13, loss = 0.76877749
Iteration 14, loss = 0.73846928
Iteration 15, loss = 0.73424083
Iteration 16, loss = 0.72915971
Iteration 17, loss = 0.71311868
Iteration 18, loss = 0.71562225
Iteration 19, loss = 0.70554292
Iteration 20, loss = 0.69908168
Iteration 21, loss = 0.69121805
Iteration 22, loss = 0.68921938
Iteration 23, loss = 0.68811711
Iteration 24, loss = 0.68903355
Iteration 25, loss = 0.67913080
Iteration 26, loss = 0.68186442
Iteration 27, loss = 0.67151847
Iteration 28, loss = 0.66687874
Iteration 29, loss = 0.67976637
Iteration 30, loss = 0.66295103
Iteration 31, loss = 0.65721518
Iteration 32, loss = 0.65093088
Iteration 33, loss = 0.64659508
Iteration 34, loss = 0.64612418
Iteration 35, loss = 0.64620409
Iteration 36, loss = 0.64492030
Iteration 37, loss = 0.63694510
Iteration 38, loss = 0.63667776
Iteration 39, loss = 0.63626857
Iteration 40, loss = 0.62834939
Iteration 41, loss = 0.62506222
Iteration 42, loss = 0.62170119
Iteration 43, loss = 0.61869266
Iteration 44, loss = 0.63141475
Iteration 45, loss = 0.62429500
Iteration 46, loss = 0.62081324
Iteration 47, loss = 0.61637959
Iteration 48, loss = 0.61610644
Iteration 49, loss = 0.62095160
Iteration 50, loss = 0.61828818
Iteration 51, loss = 0.62133062
Iteration 52, loss = 0.62141214
Iteration 53, loss = 0.61047907
Iteration 54, loss = 0.60170848
Iteration 55, loss = 0.60269493
Iteration 56, loss = 0.61051242
Iteration 57, loss = 0.59767393
Iteration 58, loss = 0.59921620
Iteration 59, loss = 0.59258669
Iteration 60, loss = 0.59193034
Iteration 61, loss = 0.59123822
Iteration 62, loss = 0.63626698
Iteration 63, loss = 0.59813686
Iteration 64, loss = 0.59831882
Iteration 65, loss = 0.60148835
Iteration 66, loss = 0.59000873
Iteration 67, loss = 0.59091756
Iteration 68, loss = 0.58490775
Iteration 69, loss = 0.58415261
Iteration 70, loss = 0.59130593
Iteration 71, loss = 0.57966515
Iteration 72, loss = 0.57921136
Iteration 73, loss = 0.58075524
Iteration 74, loss = 0.59352259
Iteration 75, loss = 0.58576798
Iteration 76, loss = 0.57665340
Iteration 77, loss = 0.57224528
Iteration 78, loss = 0.57710253
Iteration 79, loss = 0.57052789
Iteration 80, loss = 0.56754513
Iteration 81, loss = 0.56892713
Iteration 82, loss = 0.56828270
Iteration 83, loss = 0.56880425
Iteration 84, loss = 0.56363282
Iteration 85, loss = 0.56233224
Iteration 86, loss = 0.56465004
Iteration 87, loss = 0.56953301
Iteration 88, loss = 0.56289767
Iteration 89, loss = 0.55883164
Iteration 90, loss = 0.56353199
Iteration 91, loss = 0.57332512
Iteration 92, loss = 0.56003560
Iteration 93, loss = 0.55700809
Iteration 94, loss = 0.56347106
Iteration 95, loss = 0.57547671
Iteration 96, loss = 0.55798620
Iteration 97, loss = 0.55362350
Iteration 98, loss = 0.55646616
Iteration 99, loss = 0.59153803
Iteration 100, loss = 0.56519164
Iteration 101, loss = 0.56132808
Iteration 102, loss = 0.56252863
Iteration 103, loss = 0.55598945
Iteration 104, loss = 0.55502423
Iteration 105, loss = 0.55102861
Iteration 106, loss = 0.55145395
Iteration 107, loss = 0.55210522
Iteration 108, loss = 0.55457083
Iteration 109, loss = 0.54900176
Iteration 110, loss = 0.54584587
Iteration 111, loss = 0.54606514
Iteration 112, loss = 0.54344838
Iteration 113, loss = 0.54357438
Iteration 114, loss = 0.54241151
Iteration 115, loss = 0.54289668
Iteration 116, loss = 0.54554223
Iteration 117, loss = 0.54209731
Iteration 118, loss = 0.54962365
Iteration 119, loss = 0.55200788
Iteration 120, loss = 0.54913370
Iteration 121, loss = 0.54109733
Iteration 122, loss = 0.55008383
Iteration 123, loss = 0.54440477
Iteration 124, loss = 0.54099104
Iteration 125, loss = 0.55390130
Iteration 126, loss = 0.53998051
Iteration 127, loss = 0.53647786
Iteration 128, loss = 0.54060890
Iteration 129, loss = 0.53536249
Iteration 130, loss = 0.53509337
Iteration 131, loss = 0.53700820
Iteration 132, loss = 0.53742700
Iteration 133, loss = 0.53533255
Iteration 134, loss = 0.53812984
Iteration 135, loss = 0.54178668
Iteration 136, loss = 0.53666947
Iteration 137, loss = 0.53586633
Iteration 138, loss = 0.54372317
Iteration 139, loss = 0.53909527Iteration 123, loss = 0.66789191
Iteration 124, loss = 0.66781314
Iteration 125, loss = 0.67285342
Iteration 126, loss = 0.66830572
Iteration 127, loss = 0.66526119
Iteration 128, loss = 0.66786899
Iteration 129, loss = 0.67031719
Iteration 130, loss = 0.66625703
Iteration 131, loss = 0.66759223
Iteration 132, loss = 0.66744537
Iteration 133, loss = 0.66577685
Iteration 134, loss = 0.67517816
Iteration 135, loss = 0.66655798
Iteration 136, loss = 0.66420087
Iteration 137, loss = 0.66458965
Iteration 138, loss = 0.66937723
Iteration 139, loss = 0.67269948
Iteration 140, loss = 0.66884431
Iteration 141, loss = 0.66363266
Iteration 142, loss = 0.66747557
Iteration 143, loss = 0.66720204
Iteration 144, loss = 0.67681551
Iteration 145, loss = 0.66745990
Iteration 146, loss = 0.66793177
Iteration 147, loss = 0.66440356
Iteration 148, loss = 0.66570105
Iteration 149, loss = 0.66150856
Iteration 150, loss = 0.66219543
Iteration 151, loss = 0.65881617
Iteration 152, loss = 0.67297731
Iteration 153, loss = 0.66135166
Iteration 154, loss = 0.65862029
Iteration 155, loss = 0.66553408
Iteration 156, loss = 0.65786575
Iteration 157, loss = 0.65948589
Iteration 158, loss = 0.66015479
Iteration 159, loss = 0.65696859
Iteration 160, loss = 0.66056972
Iteration 161, loss = 0.66114879
Iteration 162, loss = 0.66486147
Iteration 163, loss = 0.65846202
Iteration 164, loss = 0.65588261
Iteration 165, loss = 0.65665246
Iteration 166, loss = 0.65978889
Iteration 167, loss = 0.65707429
Iteration 168, loss = 0.66208292
Iteration 169, loss = 0.65628616
Iteration 170, loss = 0.67199883
Iteration 171, loss = 0.66183469
Iteration 172, loss = 0.66622424
Iteration 173, loss = 0.65707447
Iteration 174, loss = 0.66314232
Iteration 175, loss = 0.65886954
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 1/5] END ...........alpha=0.5, momentum=0.4;, score=0.747 total time=   5.3s
Iteration 1, loss = 1.10740347
Iteration 2, loss = 0.97463625
Iteration 3, loss = 0.92433545
Iteration 4, loss = 0.88922453
Iteration 5, loss = 0.87244684
Iteration 6, loss = 0.84806690
Iteration 7, loss = 0.82677541
Iteration 8, loss = 0.81051659
Iteration 9, loss = 0.79542968
Iteration 10, loss = 0.78626203
Iteration 11, loss = 0.77946715
Iteration 12, loss = 0.76754068
Iteration 13, loss = 0.78161151
Iteration 14, loss = 0.74914942
Iteration 15, loss = 0.73779095
Iteration 16, loss = 0.73615223
Iteration 17, loss = 0.72193227
Iteration 18, loss = 0.72641562
Iteration 19, loss = 0.71390670
Iteration 20, loss = 0.70803922
Iteration 21, loss = 0.69941262
Iteration 22, loss = 0.69377791
Iteration 23, loss = 0.69683421
Iteration 24, loss = 0.69464890
Iteration 25, loss = 0.68672375
Iteration 26, loss = 0.68744357
Iteration 27, loss = 0.67864399
Iteration 28, loss = 0.67176612
Iteration 29, loss = 0.68296566
Iteration 30, loss = 0.66764695
Iteration 31, loss = 0.66362797
Iteration 32, loss = 0.65692124
Iteration 33, loss = 0.65227983
Iteration 34, loss = 0.65167435
Iteration 35, loss = 0.65228248
Iteration 36, loss = 0.64537302
Iteration 37, loss = 0.63987349
Iteration 38, loss = 0.63623333
Iteration 39, loss = 0.63569097
Iteration 40, loss = 0.63082928
Iteration 41, loss = 0.63187395
Iteration 42, loss = 0.62849789
Iteration 43, loss = 0.62431026
Iteration 44, loss = 0.63651898
Iteration 45, loss = 0.63091133
Iteration 46, loss = 0.62876579
Iteration 47, loss = 0.62126027
Iteration 48, loss = 0.61809004
Iteration 49, loss = 0.62430200
Iteration 50, loss = 0.62286405
Iteration 51, loss = 0.61721132
Iteration 52, loss = 0.62670836
Iteration 53, loss = 0.61313127
Iteration 54, loss = 0.60513144
Iteration 55, loss = 0.60055238
Iteration 56, loss = 0.61407574
Iteration 57, loss = 0.59920497
Iteration 58, loss = 0.60273620
Iteration 59, loss = 0.59464531
Iteration 60, loss = 0.59255584
Iteration 61, loss = 0.59516304
Iteration 62, loss = 0.62645467
Iteration 63, loss = 0.59568884
Iteration 64, loss = 0.58800160
Iteration 65, loss = 0.60779066
Iteration 66, loss = 0.59301751
Iteration 67, loss = 0.58996630
Iteration 68, loss = 0.58488779
Iteration 69, loss = 0.58335834
Iteration 70, loss = 0.59745710
Iteration 71, loss = 0.58184312
Iteration 72, loss = 0.58023043
Iteration 73, loss = 0.58081635
Iteration 74, loss = 0.57504549
Iteration 75, loss = 0.58375911
Iteration 76, loss = 0.57602397
Iteration 77, loss = 0.57176902
Iteration 78, loss = 0.57556124
Iteration 79, loss = 0.57274338
Iteration 80, loss = 0.56802919
Iteration 81, loss = 0.56942283
Iteration 82, loss = 0.56899602
Iteration 83, loss = 0.58709079
Iteration 84, loss = 0.57397062
Iteration 85, loss = 0.56636108
Iteration 86, loss = 0.56664471
Iteration 87, loss = 0.57339068
Iteration 88, loss = 0.56280555
Iteration 89, loss = 0.55826405
Iteration 90, loss = 0.55850581
Iteration 91, loss = 0.55963833
Iteration 92, loss = 0.55737820
Iteration 93, loss = 0.55505979
Iteration 94, loss = 0.55242985
Iteration 95, loss = 0.56770209
Iteration 96, loss = 0.55398296
Iteration 97, loss = 0.55118941
Iteration 98, loss = 0.55503058
Iteration 99, loss = 0.59486222
Iteration 100, loss = 0.56277654
Iteration 101, loss = 0.56272205
Iteration 102, loss = 0.56843564
Iteration 103, loss = 0.55690580
Iteration 104, loss = 0.55324613
Iteration 105, loss = 0.55080981
Iteration 106, loss = 0.54839978
Iteration 107, loss = 0.54826003
Iteration 108, loss = 0.54988206
Iteration 109, loss = 0.54503852
Iteration 110, loss = 0.54435665
Iteration 111, loss = 0.54296694
Iteration 112, loss = 0.54084334
Iteration 113, loss = 0.54192134
Iteration 114, loss = 0.54090605
Iteration 115, loss = 0.54008857
Iteration 116, loss = 0.54199883
Iteration 117, loss = 0.53873376
Iteration 118, loss = 0.54376014
Iteration 119, loss = 0.54914051
Iteration 120, loss = 0.54370343
Iteration 121, loss = 0.54173718
Iteration 122, loss = 0.55006225
Iteration 123, loss = 0.54256682
Iteration 124, loss = 0.53906934
Iteration 125, loss = 0.54688345
Iteration 126, loss = 0.54017615
Iteration 127, loss = 0.53552806
Iteration 128, loss = 0.53950424
Iteration 129, loss = 0.54271484
Iteration 130, loss = 0.53751784
Iteration 131, loss = 0.53690505
Iteration 132, loss = 0.53505618
Iteration 133, loss = 0.53335560
Iteration 134, loss = 0.54433223
Iteration 135, loss = 0.53383526
Iteration 136, loss = 0.53186273
Iteration 137, loss = 0.53047657
Iteration 138, loss = 0.53575250
Iteration 139, loss = 0.54111474
Iteration 140, loss = 0.53633057
Iteration 141, loss = 0.53095509
Iteration 142, loss = 0.53634646
Iteration 143, loss = 0.53539812
Iteration 144, loss = 0.54865508
Iteration 145, loss = 0.53446263
Iteration 146, loss = 0.53485128
Iteration 147, loss = 0.53122949
Iteration 148, loss = 0.53124793
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 1/5] END ...........alpha=0.3, momentum=0.2;, score=0.813 total time=   4.4s
Iteration 1, loss = 1.09925779
Iteration 2, loss = 0.96406759
Iteration 3, loss = 0.90930840
Iteration 4, loss = 0.87505098
Iteration 5, loss = 0.84260297
Iteration 6, loss = 0.81663104
Iteration 7, loss = 0.79233922
Iteration 8, loss = 0.77299304
Iteration 9, loss = 0.75378371
Iteration 10, loss = 0.73572954
Iteration 11, loss = 0.72982551
Iteration 12, loss = 0.71514641
Iteration 13, loss = 0.69787347
Iteration 14, loss = 0.68877322
Iteration 15, loss = 0.67492468
Iteration 16, loss = 0.66332741
Iteration 17, loss = 0.65591364
Iteration 18, loss = 0.64687802
Iteration 19, loss = 0.63652356
Iteration 20, loss = 0.63147715
Iteration 21, loss = 0.62379626
Iteration 22, loss = 0.61551190
Iteration 23, loss = 0.61247112
Iteration 24, loss = 0.60374311
Iteration 25, loss = 0.59958682
Iteration 26, loss = 0.59124427
Iteration 27, loss = 0.58693051
Iteration 28, loss = 0.57930736
Iteration 29, loss = 0.57739898
Iteration 30, loss = 0.56917143
Iteration 31, loss = 0.56933544
Iteration 32, loss = 0.56464511
Iteration 33, loss = 0.55660172
Iteration 34, loss = 0.55127332
Iteration 35, loss = 0.54582840          time        FC5         FC3  ...         Oz         O2         Iz
1278   0.14375  12.310972  -10.029207  ...  22.839458 -27.229542   9.433848
10388  0.66250 -44.006672  -84.033380  ... -65.664628 -25.910797 -14.483674
2369   0.32500 -12.547065   -9.232368  ... -41.827241 -27.086007 -69.082313
5002   0.18750 -50.817520  -46.033087  ...  43.824697  58.729188  12.594645
4209   0.76250 -97.823949 -100.056092  ...  22.107961  11.294952 -11.594139
...        ...        ...         ...  ...        ...        ...        ...
10955  0.88750 -39.931471  -62.527727  ... -26.142361 -46.535737 -40.216410
905    0.02500   2.783212   -0.020479  ...  -7.112876  -1.135217 -30.934973
5192   0.26875 -49.549464  -46.305856  ... -63.394509 -81.141357 -44.418189
12172  0.75000 -71.996538  -81.651254  ... -56.776927 -51.189754 -45.737939
235    0.26250  18.023034    6.938936  ... -28.194820 -60.284985 -27.842320

[7752 rows x 65 columns]
Fitting 5 folds for each of 5 candidates, totalling 25 fits
Iteration 1, loss = 1.04099755
Iteration 2, loss = 0.88611480
Iteration 3, loss = 0.80665822
Iteration 4, loss = 0.74424523
Iteration 5, loss = 0.69300934
Iteration 6, loss = 0.64817402
Iteration 7, loss = 0.60892984
Iteration 8, loss = 0.57425742
Iteration 9, loss = 0.54448667
Iteration 10, loss = 0.51803666
Iteration 11, loss = 0.49473628
Iteration 12, loss = 0.47556270
Iteration 13, loss = 0.45629149
Iteration 14, loss = 0.43995767
Iteration 15, loss = 0.42479289
Iteration 16, loss = 0.41158503
Iteration 17, loss = 0.39821320
Iteration 18, loss = 0.38753578
Iteration 19, loss = 0.37744277
Iteration 20, loss = 0.36737490
Iteration 21, loss = 0.35840832
Iteration 22, loss = 0.35037988
Iteration 23, loss = 0.34305059
Iteration 24, loss = 0.33561683
Iteration 25, loss = 0.32935976
Iteration 26, loss = 0.32312041
Iteration 27, loss = 0.31761907
Iteration 28, loss = 0.31161843
Iteration 29, loss = 0.30700047
Iteration 30, loss = 0.30282719
Iteration 31, loss = 0.29769704
Iteration 32, loss = 0.29344766
Iteration 33, loss = 0.28903800
Iteration 34, loss = 0.28527092
Iteration 35, loss = 0.28165537
Iteration 36, loss = 0.27825016
Iteration 37, loss = 0.27530393
Iteration 38, loss = 0.27203539
Iteration 39, loss = 0.26916751
Iteration 40, loss = 0.26667304
Iteration 41, loss = 0.26365621
Iteration 42, loss = 0.26177766
Iteration 43, loss = 0.25888284
Iteration 44, loss = 0.25636274
Iteration 45, loss = 0.25408098
Iteration 46, loss = 0.25291512
Iteration 47, loss = 0.24936213
Iteration 48, loss = 0.24769931
Iteration 49, loss = 0.24540312
Iteration 50, loss = 0.24374576
Iteration 51, loss = 0.24196184
Iteration 52, loss = 0.24070470
Iteration 53, loss = 0.23866037
Iteration 54, loss = 0.23699199
Iteration 55, loss = 0.23468030
Iteration 56, loss = 0.23382421
Iteration 57, loss = 0.23250713
Iteration 58, loss = 0.23064071
Iteration 59, loss = 0.22970237
Iteration 60, loss = 0.22789417
Iteration 61, loss = 0.22630366
Iteration 62, loss = 0.22539217
Iteration 63, loss = 0.22449429
Iteration 64, loss = 0.22331760
Iteration 65, loss = 0.22159268
Iteration 66, loss = 0.22025383
Iteration 67, loss = 0.21996956
Iteration 68, loss = 0.21817851
Iteration 69, loss = 0.21698248
Iteration 70, loss = 0.21615357
Iteration 71, loss = 0.21529879
Iteration 72, loss = 0.21456995
Iteration 73, loss = 0.21388356
Iteration 74, loss = 0.21243318
Iteration 75, loss = 0.21267768
Iteration 76, loss = 0.21100784
Iteration 77, loss = 0.20997708
Iteration 78, loss = 0.20900418
Iteration 79, loss = 0.20759754
Iteration 80, loss = 0.20702554
Iteration 81, loss = 0.20727070
Iteration 82, loss = 0.20617972
Iteration 83, loss = 0.20470617
Iteration 84, loss = 0.20409840
Iteration 85, loss = 0.20325849
Iteration 86, loss = 0.20270554
Iteration 87, loss = 0.20204078
Iteration 88, loss = 0.20159880
Iteration 89, loss = 0.20037683
Iteration 90, loss = 0.20058369
Iteration 91, loss = 0.19955642
Iteration 92, loss = 0.19877079
Iteration 93, loss = 0.19830363
Iteration 94, loss = 0.19766597
Iteration 95, loss = 0.19684083
Iteration 96, loss = 0.19664048
Iteration 97, loss = 0.19592134
Iteration 98, loss = 0.19546950
Iteration 99, loss = 0.19499702
Iteration 100, loss = 0.19490988
Iteration 101, loss = 0.19406615
Iteration 102, loss = 0.19336971
Iteration 103, loss = 0.19296223
Iteration 104, loss = 0.19308115
Iteration 105, loss = 0.19300245
Iteration 106, loss = 0.19186069
Iteration 107, loss = 0.19181015
Iteration 108, loss = 0.19055639
Iteration 109, loss = 0.19064919
Iteration 110, loss = 0.19047257
Iteration 111, loss = 0.19099434
Iteration 112, loss = 0.19001290
Iteration 113, loss = 0.18905815
Iteration 114, loss = 0.18895879
Iteration 115, loss = 0.18847995
Iteration 116, loss = 0.18798412
Iteration 117, loss = 0.18753488
Iteration 118, loss = 0.18799814
Iteration 119, loss = 0.18715732
Iteration 120, loss = 0.18700506
Iteration 121, loss = 0.18709417
Iteration 122, loss = 0.18617867
Iteration 123, loss = 0.18640031
Iteration 124, loss = 0.18558369
Iteration 125, loss = 0.18577639
Iteration 126, loss = 0.18538244
Iteration 127, loss = 0.18557681
Iteration 128, loss = 0.18433405
Iteration 129, loss = 0.18461479
Iteration 130, loss = 0.18468648
Iteration 131, loss = 0.18428078
Iteration 132, loss = 0.18405484
Iteration 133, loss = 0.18365842
Iteration 134, loss = 0.18364943
Iteration 135, loss = 0.18345616
Iteration 136, loss = 0.18282657
Iteration 137, loss = 0.18236853
Iteration 138, loss = 0.18268135
Iteration 139, loss = 0.18265768
Iteration 140, loss = 0.18228615
Iteration 141, loss = 0.18210321
Iteration 142, loss = 0.18188859
Iteration 143, loss = 0.18257474
Iteration 144, loss = 0.18254674
Iteration 145, loss = 0.18107998
Iteration 146, loss = 0.18117098
Iteration 147, loss = 0.18084807
Iteration 148, loss = 0.18063873
Iteration 149, loss = 0.18089508
Iteration 150, loss = 0.18031265
Iteration 151, loss = 0.17993385
Iteration 152, loss = 0.18031979
Iteration 153, loss = 0.17968814
Iteration 154, loss = 0.18039557
Iteration 155, loss = 0.18018869
Iteration 156, loss = 0.17991995
Iteration 157, loss = 0.17935198
Iteration 158, loss = 0.17948511
Iteration 159, loss = 0.17916532
Iteration 160, loss = 0.17887553
Iteration 161, loss = 0.17866596
Iteration 162, loss = 0.17947930
Iteration 163, loss = 0.17868622
Iteration 164, loss = 0.17892747
Iteration 165, loss = 0.17850732
Iteration 166, loss = 0.17807252
Iteration 167, loss = 0.17810452
Iteration 168, loss = 0.17811262
Iteration 169, loss = 0.17794171
Iteration 170, loss = 0.17781340
Iteration 171, loss = 0.17854770
Iteration 172, loss = 0.17803088
Iteration 173, loss = 0.17775323
Iteration 174, loss = 0.17764017
Iteration 175, loss = 0.17772475
Iteration 176, loss = 0.17772127
Iteration 177, loss = 0.17706788
Iteration 178, loss = 0.17744773
Iteration 179, loss = 0.17641652
Iteration 180, loss = 0.17700504
Iteration 181, loss = 0.17703508
Iteration 182, loss = 0.17708324
Iteration 183, loss = 0.17623805
Iteration 184, loss = 0.17631403
Iteration 185, loss = 0.17602635
Iteration 186, loss = 0.17632146
Iteration 187, loss = 0.17670469
Iteration 188, loss = 0.17611990
Iteration 189, loss = 0.17606857
Iteration 190, loss = 0.17604854
Iteration 191, loss = 0.17587508
Iteration 192, loss = 0.17598781
Iteration 193, loss = 0.17622732
Iteration 194, loss = 0.17613701
Iteration 195, loss = 0.17609272
Iteration 196, loss = 0.17558191
Iteration 197, loss = 0.17599353
Iteration 198, loss = 0.17562184
Iteration 199, loss = 0.17569668
Iteration 200, loss = 0.17531378
Iteration 201, loss = 0.17529748
Iteration 202, loss = 0.17547668
Iteration 203, loss = 0.17499877
Iteration 204, loss = 0.17483247
Iteration 205, loss = 0.17483092
Iteration 206, loss = 0.17492339
Iteration 207, loss = 0.17431399
Iteration 208, loss = 0.17510306
Iteration 209, loss = 0.17481058
Iteration 210, loss = 0.17513217
Iteration 211, loss = 0.17482813
Iteration 212, loss = 0.17468686
Iteration 213, loss = 0.17448507
Iteration 214, loss = 0.17533003
Iteration 215, loss = 0.17458273
Iteration 216, loss = 0.17459040
Iteration 217, loss = 0.17444404
Iteration 218, loss = 0.17419629
Iteration 219, loss = 0.17414939
Iteration 220, loss = 0.17441148
Iteration 221, loss = 0.17423078
Iteration 222, loss = 0.17411840
Iteration 223, loss = 0.17386631
Iteration 224, loss = 0.17464551
Iteration 225, loss = 0.17409883
Iteration 226, loss = 0.17383344
Iteration 227, loss = 0.17395319
Iteration 228, loss = 0.17399796
Iteration 229, loss = 0.17401216
Iteration 230, loss = 0.17381298
Iteration 231, loss = 0.17386346
Iteration 232, loss = 0.17324453
Iteration 233, loss = 0.17378754
Iteration 234, loss = 0.17330565
Iteration 235, loss = 0.17328846
Iteration 236, loss = 0.17331443
Iteration 237, loss = 0.17300219
Iteration 238, loss = 0.17342797
Iteration 239, loss = 0.17283868
Iteration 240, loss = 0.17327570
Iteration 241, loss = 0.17366248
Iteration 242, loss = 0.17327942
Iteration 243, loss = 0.17373347
Iteration 244, loss = 0.17337292
Iteration 245, loss = 0.17331376
Iteration 246, loss = 0.17435665
Iteration 247, loss = 0.17359089
Iteration 248, loss = 0.17288338
Iteration 249, loss = 0.17302518
Iteration 250, loss = 0.17273782
Iteration 251, loss = 0.17287157
Iteration 252, loss = 0.17304652
Iteration 253, loss = 0.17285131
Iteration 254, loss = 0.17369804
Iteration 255, loss = 0.17315514
Iteration 256, loss = 0.17269430
Iteration 257, loss = 0.17263370
Iteration 258, loss = 0.17275979
Iteration 259, loss = 0.17252005
Iteration 260, loss = 0.17270239
Iteration 261, loss = 0.17308917
Iteration 262, loss = 0.17218542
Iteration 263, loss = 0.17343960
Iteration 264, loss = 0.17231605
Iteration 265, loss = 0.17244455
Iteration 266, loss = 0.17281567
Iteration 267, loss = 0.17360408
Iteration 268, loss = 0.17232228
Iteration 269, loss = 0.17271270
Iteration 270, loss = 0.17201525
Iteration 271, loss = 0.17207288
Iteration 272, loss = 0.17236087
Iteration 273, loss = 0.17209567
Iteration 274, loss = 0.17246129
Iteration 275, loss = 0.17246846
Iteration 276, loss = 0.17212194
Iteration 277, loss = 0.17224493
Iteration 278, loss = 0.17190350
Iteration 279, loss = 0.17205079
Iteration 280, loss = 0.17239387
Iteration 281, loss = 0.17166905
Iteration 282, loss = 0.17249263
Iteration 283, loss = 0.17228582
Iteration 284, loss = 0.17181757
Iteration 285, loss = 0.17173147
Iteration 286, loss = 0.17167997
Iteration 287, loss = 0.17181741
Iteration 288, loss = 0.17160743
Iteration 289, loss = 0.17145517
Iteration 290, loss = 0.17190553
Iteration 291, loss = 0.17182077
Iteration 292, loss = 0.17145307
Iteration 293, loss = 0.17171384
Iteration 294, loss = 0.17116579
Iteration 295, loss = 0.17137668
Iteration 296, loss = 0.17156025
Iteration 297, loss = 0.17198483
Iteration 298, loss = 0.17129582
Iteration 299, loss = 0.17139204
Iteration 300, loss = 0.17145430
Iteration 301, loss = 0.17188325
Iteration 302, loss = 0.17134707
Iteration 303, loss = 0.17112050
Iteration 304, loss = 0.17166343
Iteration 305, loss = 0.17188823
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
here
0.6838846972335074

Iteration 140, loss = 0.53543664
Iteration 141, loss = 0.53247067
Iteration 142, loss = 0.53942003
Iteration 143, loss = 0.53841907
Iteration 144, loss = 0.53143169
Iteration 145, loss = 0.52892377
Iteration 146, loss = 0.54602817
Iteration 147, loss = 0.54353423
Iteration 148, loss = 0.53265985
Iteration 149, loss = 0.52976161
Iteration 150, loss = 0.53131761
Iteration 151, loss = 0.52804948
Iteration 152, loss = 0.54510047
Iteration 153, loss = 0.53323060
Iteration 154, loss = 0.52851310
Iteration 155, loss = 0.53093122
Iteration 156, loss = 0.53115724
Iteration 157, loss = 0.52810453
Iteration 158, loss = 0.52870734
Iteration 159, loss = 0.52615093
Iteration 160, loss = 0.52722912
Iteration 161, loss = 0.53675040
Iteration 162, loss = 0.53177957
Iteration 163, loss = 0.52654923
Iteration 164, loss = 0.52433455
Iteration 165, loss = 0.52721762
Iteration 166, loss = 0.53469264
Iteration 167, loss = 0.52732326
Iteration 168, loss = 0.52987648
Iteration 169, loss = 0.52481365
Iteration 170, loss = 0.53873557
Iteration 171, loss = 0.53342550
Iteration 172, loss = 0.54165683
Iteration 173, loss = 0.52771699
Iteration 174, loss = 0.53245225
Iteration 175, loss = 0.52569713
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 2/5] END ...........alpha=0.3, momentum=0.2;, score=0.815 total time=   5.0s

Iteration 112, loss = 0.43512614
Iteration 113, loss = 0.43048564
Iteration 114, loss = 0.42926470
Iteration 115, loss = 0.43389678
Iteration 116, loss = 0.43425260
Iteration 117, loss = 0.42950640
Iteration 118, loss = 0.43705781
Iteration 119, loss = 0.43207669
Iteration 120, loss = 0.42964325
Iteration 121, loss = 0.43271197
Iteration 122, loss = 0.42689395
Iteration 123, loss = 0.42559639
Iteration 124, loss = 0.42662661
Iteration 125, loss = 0.42533382
Iteration 126, loss = 0.42834480
Iteration 127, loss = 0.42508461
Iteration 128, loss = 0.42578296
Iteration 129, loss = 0.42219145
Iteration 130, loss = 0.42176001
Iteration 131, loss = 0.42399590
Iteration 132, loss = 0.42553252
Iteration 133, loss = 0.42237966
Iteration 134, loss = 0.42240246
Iteration 135, loss = 0.42172471
Iteration 136, loss = 0.41998386
Iteration 137, loss = 0.41933281
Iteration 138, loss = 0.42415805
Iteration 139, loss = 0.42452677
Iteration 140, loss = 0.42069478
Iteration 141, loss = 0.45081713
Iteration 142, loss = 0.42958527
Iteration 143, loss = 0.42537907
Iteration 144, loss = 0.43129031
Iteration 145, loss = 0.42285731
Iteration 146, loss = 0.42946420
Iteration 147, loss = 0.41972081
Iteration 148, loss = 0.43586954
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 3/5] END ...........alpha=0.3, momentum=0.2;, score=0.852 total time=   4.3s

Iteration 44, loss = 0.52536104
Iteration 45, loss = 0.52164356
Iteration 46, loss = 0.51693931
Iteration 47, loss = 0.51847856
Iteration 48, loss = 0.51056544
Iteration 49, loss = 0.51221579
Iteration 50, loss = 0.50578424
Iteration 51, loss = 0.50186262
Iteration 52, loss = 0.50244746
Iteration 53, loss = 0.49701628
Iteration 54, loss = 0.50512579
Iteration 55, loss = 0.49586723
Iteration 56, loss = 0.49420863
Iteration 57, loss = 0.49709765
Iteration 58, loss = 0.49187995
Iteration 59, loss = 0.48540409
Iteration 60, loss = 0.48875462
Iteration 61, loss = 0.48422205
Iteration 62, loss = 0.48423839
Iteration 63, loss = 0.48691978
Iteration 64, loss = 0.47693443
Iteration 65, loss = 0.47417083
Iteration 66, loss = 0.47271656
Iteration 67, loss = 0.47121912
Iteration 68, loss = 0.46866830
Iteration 69, loss = 0.47364087
Iteration 70, loss = 0.47033791
Iteration 71, loss = 0.46859920
Iteration 72, loss = 0.46598346
Iteration 73, loss = 0.47363452
Iteration 74, loss = 0.46392523
Iteration 75, loss = 0.46042129
Iteration 76, loss = 0.45817083
Iteration 77, loss = 0.45796205
Iteration 78, loss = 0.46610707
Iteration 79, loss = 0.47094717
Iteration 80, loss = 0.45940838
Iteration 81, loss = 0.45856748
Iteration 82, loss = 0.45790796
Iteration 83, loss = 0.45256749
Iteration 84, loss = 0.45330840
Iteration 85, loss = 0.44970568
Iteration 86, loss = 0.45388951
Iteration 87, loss = 0.45159034
Iteration 88, loss = 0.45050807
Iteration 89, loss = 0.45043616
Iteration 90, loss = 0.44986250
Iteration 91, loss = 0.44677230
Iteration 92, loss = 0.44558412
Iteration 93, loss = 0.44601274
Iteration 94, loss = 0.44340806
Iteration 95, loss = 0.44288758
Iteration 96, loss = 0.44096742
Iteration 97, loss = 0.44020920
Iteration 98, loss = 0.44721508
Iteration 99, loss = 0.43948733
Iteration 100, loss = 0.44044100
Iteration 101, loss = 0.44065864
Iteration 102, loss = 0.45862518
Iteration 103, loss = 0.44809532
Iteration 104, loss = 0.44336065
Iteration 105, loss = 0.43595894
Iteration 106, loss = 0.43413105
Iteration 107, loss = 0.43338255
Iteration 108, loss = 0.43805197
Iteration 109, loss = 0.43430147
Iteration 110, loss = 0.43071119
Iteration 111, loss = 0.43500267
Iteration 112, loss = 0.43586323
Iteration 113, loss = 0.43150355
Iteration 114, loss = 0.43176953
Iteration 115, loss = 0.43629503
Iteration 116, loss = 0.43354564
Iteration 117, loss = 0.43940115
Iteration 118, loss = 0.43918945
Iteration 119, loss = 0.43339544
Iteration 120, loss = 0.43196238
Iteration 121, loss = 0.43071869
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 4/5] END ...........alpha=0.3, momentum=0.2;, score=0.859 total time=   3.1s

Iteration 36, loss = 0.54424343
Iteration 37, loss = 0.54150537
Iteration 38, loss = 0.53810020
Iteration 39, loss = 0.53293129
Iteration 40, loss = 0.53004412
Iteration 41, loss = 0.52532466
Iteration 42, loss = 0.52222493
Iteration 43, loss = 0.51769414
Iteration 44, loss = 0.52788060
Iteration 45, loss = 0.51522419
Iteration 46, loss = 0.50905685
Iteration 47, loss = 0.50920070
Iteration 48, loss = 0.50341687
Iteration 49, loss = 0.50467603
Iteration 50, loss = 0.50131957
Iteration 51, loss = 0.49703694
Iteration 52, loss = 0.49633180
Iteration 53, loss = 0.48986961
Iteration 54, loss = 0.50018827
Iteration 55, loss = 0.49046588
Iteration 56, loss = 0.48723772
Iteration 57, loss = 0.48946494
Iteration 58, loss = 0.48665633
Iteration 59, loss = 0.47918022
Iteration 60, loss = 0.48465607
Iteration 61, loss = 0.47651053
Iteration 62, loss = 0.47681289
Iteration 63, loss = 0.47578134
Iteration 64, loss = 0.47026935
Iteration 65, loss = 0.46759712
Iteration 66, loss = 0.46602666
Iteration 67, loss = 0.46420496
Iteration 68, loss = 0.46312582
Iteration 69, loss = 0.46780967
Iteration 70, loss = 0.46427801
Iteration 71, loss = 0.46268271
Iteration 72, loss = 0.46004780
Iteration 73, loss = 0.46828988
Iteration 74, loss = 0.46010885
Iteration 75, loss = 0.45477974
Iteration 76, loss = 0.45200886
Iteration 77, loss = 0.45478807
Iteration 78, loss = 0.46544762
Iteration 79, loss = 0.46779266
Iteration 80, loss = 0.45146877
Iteration 81, loss = 0.45215881
Iteration 82, loss = 0.45247310
Iteration 83, loss = 0.44681507
Iteration 84, loss = 0.44665866
Iteration 85, loss = 0.44475859
Iteration 86, loss = 0.44895376
Iteration 87, loss = 0.44859963
Iteration 88, loss = 0.44276814
Iteration 89, loss = 0.43989364
Iteration 90, loss = 0.44451825
Iteration 91, loss = 0.44466802
Iteration 92, loss = 0.45231831
Iteration 93, loss = 0.44283345
Iteration 94, loss = 0.43876209
Iteration 95, loss = 0.44117947
Iteration 96, loss = 0.44218180
Iteration 97, loss = 0.43632220
Iteration 98, loss = 0.44155312
Iteration 99, loss = 0.43546611
Iteration 100, loss = 0.43580038
Iteration 101, loss = 0.43723416
Iteration 102, loss = 0.44828980
Iteration 103, loss = 0.44329674
Iteration 104, loss = 0.43909066
Iteration 105, loss = 0.43261741
Iteration 106, loss = 0.43018467
Iteration 107, loss = 0.42833589
Iteration 108, loss = 0.43184335
Iteration 109, loss = 0.42784859
Iteration 110, loss = 0.42587578
Iteration 111, loss = 0.42838889
Iteration 112, loss = 0.42771256
Iteration 113, loss = 0.42800500
Iteration 114, loss = 0.42540451
Iteration 115, loss = 0.42879622
Iteration 116, loss = 0.42759970
Iteration 117, loss = 0.43506424
Iteration 118, loss = 0.44507300
Iteration 119, loss = 0.42971458
Iteration 120, loss = 0.42814418
Iteration 121, loss = 0.42556816
Iteration 122, loss = 0.42486955
Iteration 123, loss = 0.42679610
Iteration 124, loss = 0.42849553
Iteration 125, loss = 0.42363075
Iteration 126, loss = 0.42413254
Iteration 127, loss = 0.42570686
Iteration 128, loss = 0.42507236
Iteration 129, loss = 0.41957437
Iteration 130, loss = 0.41878570
Iteration 131, loss = 0.41914309
Iteration 132, loss = 0.42058093
Iteration 133, loss = 0.41752875
Iteration 134, loss = 0.41758903
Iteration 135, loss = 0.42024563
Iteration 136, loss = 0.41709111
Iteration 137, loss = 0.41711822
Iteration 138, loss = 0.42248762
Iteration 139, loss = 0.42009446
Iteration 140, loss = 0.41611751
Iteration 141, loss = 0.41995939
Iteration 142, loss = 0.42108818
Iteration 143, loss = 0.42164401
Iteration 144, loss = 0.42317556
Iteration 145, loss = 0.41979445
Iteration 146, loss = 0.41837604
Iteration 147, loss = 0.42598100
Iteration 148, loss = 0.44002310
Iteration 149, loss = 0.42206925
Iteration 150, loss = 0.41488707
Iteration 151, loss = 0.41309293
Iteration 152, loss = 0.41773258
Iteration 153, loss = 0.41810009
Iteration 154, loss = 0.41568813
Iteration 155, loss = 0.41485867
Iteration 156, loss = 0.41201013
Iteration 157, loss = 0.41391306
Iteration 158, loss = 0.41353866
Iteration 159, loss = 0.41303820
Iteration 160, loss = 0.41115373
Iteration 161, loss = 0.42396969
Iteration 162, loss = 0.41313286
Iteration 163, loss = 0.41281247
Iteration 164, loss = 0.41050976
Iteration 165, loss = 0.41151846
Iteration 166, loss = 0.41265006
Iteration 167, loss = 0.42337984
Iteration 168, loss = 0.41647535
Iteration 169, loss = 0.41147461
Iteration 170, loss = 0.41314129
Iteration 171, loss = 0.41437012
Iteration 172, loss = 0.41100214
Iteration 173, loss = 0.41249236
Iteration 174, loss = 0.41280086
Iteration 175, loss = 0.41071794
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
[CV 5/5] END ...........alpha=0.3, momentum=0.2;, score=0.869 total time=   4.0s
